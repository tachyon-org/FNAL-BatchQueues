{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae386ba8-8e9a-46fc-af97-78a1bc8e4794",
   "metadata": {},
   "source": [
    "## Perlmutter Anon. Code ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55db265e-cea9-46b3-909c-a60b3481b2e0",
   "metadata": {},
   "source": [
    "#### Anon. These fields: User, Account ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e66c1a-89c4-4d57-b73a-516b2190adde",
   "metadata": {},
   "source": [
    "#### Imports ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62f3c4ec-2115-4015-9f62-7b04c726c3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Anonymize CSV job data.\n",
    "\n",
    "- Reads input files\n",
    "- Writes anonymized copies\n",
    "- Replaces Seq.:\n",
    "    User    -> UR_1, UR_2, ...\n",
    "    Account -> AC_1, AC_2, ...\n",
    "  Global mapping across ALL processed files.\n",
    "- Writes JSON mapping for tracing.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3c29e4-ffab-410b-9de0-f629441174a5",
   "metadata": {},
   "source": [
    "#### Token Mapper ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4580c151-a7eb-40a4-aea4-928af5e1fab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class TokenRecord:\n",
    "    token: str\n",
    "    count: int\n",
    "    valid: bool\n",
    "\n",
    "\n",
    "class GarbleTokenMapper:\n",
    "    def __init__(self, prefix: str = \"UR_\", start: int = 1):\n",
    "        self.prefix = str(prefix)\n",
    "        self.start = int(start)\n",
    "        self._by_orig: Dict[str, TokenRecord] = {}\n",
    "        self._token_to_orig: Dict[str, str] = {}\n",
    "        self._counter = self.start - 1\n",
    "\n",
    "    def _next_token(self) -> str:\n",
    "        self._counter += 1\n",
    "        return f\"{self.prefix}{self._counter}\"\n",
    "\n",
    "    def add(self, original: str, *, valid: bool = True) -> str:\n",
    "        if original is None:\n",
    "            return original\n",
    "        key = str(original).strip()\n",
    "        if not key:\n",
    "            return original\n",
    "        if key in self._by_orig:\n",
    "            rec = self._by_orig[key]\n",
    "            rec.count += 1\n",
    "            return rec.token\n",
    "        tok = self._next_token()\n",
    "        rec = TokenRecord(token=tok, count=1, valid=bool(valid))\n",
    "        self._by_orig[key] = rec\n",
    "        self._token_to_orig[tok] = key\n",
    "        return tok\n",
    "\n",
    "    def to_jsonable(self) -> dict:\n",
    "        entries = []\n",
    "        for orig, rec in self._by_orig.items():\n",
    "            item = asdict(rec)\n",
    "            item[\"original\"] = orig\n",
    "            entries.append(item)\n",
    "        return {\n",
    "            \"prefix\": self.prefix,\n",
    "            \"start\": self.start,\n",
    "            \"counter\": self._counter,\n",
    "            \"entries\": entries,\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b60ca-954d-4621-9bbe-346e4518229d",
   "metadata": {},
   "source": [
    "#### Core Processing ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "451efd97-4015-40aa-8c6d-56921281fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def anonymize_dataframe(df: pd.DataFrame,\n",
    "                        user_mapper: GarbleTokenMapper,\n",
    "                        account_mapper: GarbleTokenMapper) -> pd.DataFrame:\n",
    "    out_df = df.copy()\n",
    "\n",
    "    if \"User\" in out_df.columns:\n",
    "        out_df[\"User\"] = [\n",
    "            user_mapper.add(str(v).strip(), valid=True) if str(v).strip() else str(v).strip()\n",
    "            for v in out_df[\"User\"]\n",
    "        ]\n",
    "\n",
    "    if \"Account\" in out_df.columns:\n",
    "        out_df[\"Account\"] = [\n",
    "            account_mapper.add(str(v).strip(), valid=True) if str(v).strip() else str(v).strip()\n",
    "            for v in out_df[\"Account\"]\n",
    "        ]\n",
    "\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def process_file(input_path: Path,\n",
    "                 output_path: Path,\n",
    "                 user_mapper: GarbleTokenMapper,\n",
    "                 account_mapper: GarbleTokenMapper) -> None:\n",
    " \n",
    "    df = pd.read_csv(input_path, dtype={\"Reservation\": str, \"FailedNode\": str}, low_memory=False)\n",
    "\n",
    "    df_anon = anonymize_dataframe(df, user_mapper, account_mapper)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_anon.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "def build_mapping_json(user_mapper: GarbleTokenMapper,\n",
    "                       account_mapper: GarbleTokenMapper) -> dict:\n",
    "    return {\n",
    "        \"users\": user_mapper.to_jsonable(),\n",
    "        \"accounts\": account_mapper.to_jsonable(),\n",
    "        \"meta\": {\n",
    "            \"description\": \"Mapping from original User/Account to anonymized names (UR_n / AC_n)\",\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6811e27e-70ae-4c64-b6f7-c946b26f15cf",
   "metadata": {},
   "source": [
    "#### Directory ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d331022-96a3-4b0a-bc4b-df30a89d2330",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected input root:\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_root)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 50\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 37\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         rel_path \u001b[38;5;241m=\u001b[39m in_file\u001b[38;5;241m.\u001b[39mrelative_to(input_root)\n\u001b[1;32m     35\u001b[0m         out_file \u001b[38;5;241m=\u001b[39m output_root \u001b[38;5;241m/\u001b[39m rel_path\n\u001b[0;32m---> 37\u001b[0m         \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_mapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccount_mapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m mapping \u001b[38;5;241m=\u001b[39m build_mapping_json(user_mapper, account_mapper)\n\u001b[1;32m     40\u001b[0m mapping_path \u001b[38;5;241m=\u001b[39m output_root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperlmutter_user_account_map.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[13], line 26\u001b[0m, in \u001b[0;36mprocess_file\u001b[0;34m(input_path, output_path, user_mapper, account_mapper)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess_file\u001b[39m(input_path: Path,\n\u001b[1;32m     22\u001b[0m                  output_path: Path,\n\u001b[1;32m     23\u001b[0m                  user_mapper: GarbleTokenMapper,\n\u001b[1;32m     24\u001b[0m                  account_mapper: GarbleTokenMapper) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReservation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFailedNode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     df_anon \u001b[38;5;241m=\u001b[39m anonymize_dataframe(df, user_mapper, account_mapper)\n\u001b[1;32m     29\u001b[0m     output_path\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/venv/py310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/py310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/py310/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1964\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m col_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1959\u001b[0m         d \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1960\u001b[0m             dtype[k]\n\u001b[1;32m   1961\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m pandas_dtype(dtype[k]) \u001b[38;5;129;01min\u001b[39;00m (np\u001b[38;5;241m.\u001b[39mstr_, np\u001b[38;5;241m.\u001b[39mobject_)\n\u001b[1;32m   1962\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1963\u001b[0m         )\n\u001b[0;32m-> 1964\u001b[0m         new_col_dict[k] \u001b[38;5;241m=\u001b[39m \u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1965\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1966\u001b[0m     new_col_dict \u001b[38;5;241m=\u001b[39m col_dict\n",
      "File \u001b[0;32m~/venv/py310/lib/python3.10/site-packages/pandas/core/series.py:584\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    582\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m     manager \u001b[38;5;241m=\u001b[39m _get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/venv/py310/lib/python3.10/site-packages/pandas/core/construction.py:625\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    621\u001b[0m             subarr \u001b[38;5;241m=\u001b[39m subarr\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    624\u001b[0m         \u001b[38;5;66;03m# we will try to copy by-definition here\u001b[39;00m\n\u001b[0;32m--> 625\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m \u001b[43m_try_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# e.g. dask array GH#38645\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m copy:\n",
      "File \u001b[0;32m~/venv/py310/lib/python3.10/site-packages/pandas/core/construction.py:806\u001b[0m, in \u001b[0;36m_try_cast\u001b[0;34m(arr, dtype, copy)\u001b[0m\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    805\u001b[0m         shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(arr),)\n\u001b[0;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensure_string_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_na_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    807\u001b[0m         shape\n\u001b[1;32m    808\u001b[0m     )\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m maybe_cast_to_datetime(arr, dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main() -> None:\n",
    "\n",
    "    try:\n",
    "        script_dir = Path(__file__).resolve().parent\n",
    "    except NameError:\n",
    "        script_dir = Path.cwd()\n",
    "\n",
    "\n",
    "    input_root = (script_dir.parent / \"perlmutter_data\").resolve()\n",
    "\n",
    "    output_root = script_dir / \"PelmutterOutput\"\n",
    "    output_root.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    user_mapper = GarbleTokenMapper(prefix=\"UR_\", start=1)\n",
    "    account_mapper = GarbleTokenMapper(prefix=\"AC_\", start=1)\n",
    "\n",
    "    if not input_root.exists():\n",
    "        raise FileNotFoundError(f\"Input root does not exist: {input_root}\")\n",
    "\n",
    "    for year_dir in sorted(input_root.iterdir()):\n",
    "        if not year_dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        rel_year = year_dir.relative_to(input_root)\n",
    "        out_year_dir = output_root / rel_year\n",
    "        out_year_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for in_file in sorted(year_dir.iterdir()):\n",
    "            if not in_file.is_file():\n",
    "                continue\n",
    "            if in_file.suffix.lower() not in {\".csv\", \".txt\"}:\n",
    "                continue\n",
    "\n",
    "            rel_path = in_file.relative_to(input_root)\n",
    "            out_file = output_root / rel_path\n",
    "\n",
    "            process_file(in_file, out_file, user_mapper, account_mapper)\n",
    "\n",
    "    mapping = build_mapping_json(user_mapper, account_mapper)\n",
    "    mapping_path = output_root / \"perlmutter_user_account_map.json\"\n",
    "    with mapping_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(mapping, f, indent=2)\n",
    "\n",
    "    print(f\"Anonymized files written under: {output_root}\")\n",
    "    print(f\"Mapping written to: {mapping_path}\")\n",
    "    print(\"Script directory:\", script_dir)\n",
    "    print(\"Expected input root:\", input_root)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3fa7a1-20b5-4b6c-9461-156c00594073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
