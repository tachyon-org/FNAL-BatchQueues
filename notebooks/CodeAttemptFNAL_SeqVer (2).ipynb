{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "a7b80bbb-dd2a-4ed6-8f12-7627ad56a48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nathan/git/fnal-proj/notebooks/CodeAttemptFNAL_SeqVer.py\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Replace with your filename if needed\n",
    "file_path = Path(\"CodeAttemptFNAL_SeqVer.py\").resolve()\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f4f76f-e634-488f-88fa-09bbf36e7834",
   "metadata": {},
   "source": [
    "##### Note: Code split between two parts for me. First part is gathering data to put in a way to use, second part is transforming it. Also, headers are below code. ##### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1ca27c-51a0-419c-9235-22031ee4dfcb",
   "metadata": {},
   "source": [
    "## Methods ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f2d68-5037-416b-ad30-b653ebedbbe4",
   "metadata": {},
   "source": [
    "\n",
    "(Garble Methods)\n",
    "\n",
    "\n",
    "\n",
    "add — returns a stable token for an original and increments its count.\n",
    "\n",
    "original_from_token — reverse-maps a token back to the original string.\n",
    "\n",
    "record_from_token — fetches the UserRecord associated with a token.\n",
    "\n",
    "export_to_json — saves the mapper’s current state to disk.\n",
    "\n",
    "load_from_json — restores the mapper’s state from a saved JSON file.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(Helper Methods)\n",
    "is_valid_user — checks that a user value is non-null and non-empty.\n",
    "\n",
    "is_valid_ipv4 — validates IPv4 dotted-quad format and range.\n",
    "\n",
    "to_jagged_array — builds [[original, token, count, valid], ...] for non-anonymous use.\n",
    "\n",
    "dump_json — writes a Python object to a pretty-printed JSON file.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(Data Methods)\n",
    "load_dataframe — selects required columns from all Parquet files in DATA_DIR.\n",
    "\n",
    "build_obfuscations — iterates rows to create user/IP token maps with counts/validity.\n",
    "\n",
    "make_summary_payload — returns anonymized users/IPs jagged arrays plus meta.\n",
    "\n",
    "failed_users_payload — returns anonymized records for users with failed jobs plus meta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "825b86e0-dcf0-4f5c-8084-62db4becd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import secrets\n",
    "from dataclasses import dataclass, asdict #asdict for json\n",
    "from typing import Dict, Optional, List, Tuple #tuple for serializing\n",
    "#For transforming data into what we want.\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "#For reading data and putting it into something usable.\n",
    "\n",
    "from pathlib import Path\n",
    "from textwrap import fill, indent\n",
    "#For readable texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "0cd0ac9a-fdec-490d-b524-24fd8ebe5740",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "OUTPUT_DIR = \"./Output\"\n",
    "HUMAN_WRAP = 100 # wrap width for text output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4385f10c-4a5f-41b2-ba35-5fd7bac11178",
   "metadata": {},
   "source": [
    "### Import stuff ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "06a924c1-dddc-4b77-8ca3-f6af01587dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_COL   = \"User\"\n",
    "IP_COL     = \"JobsubClientIpAddress\"\n",
    "FAILED_COL = \"DAG_NodesFailed\"  # “boolean-ish”\n",
    "NUM_STARTS_COL     = \"NumJobStarts\"\n",
    "NUM_COMPLETIONS_COL= \"NumJobCompletions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bb0771-4508-4180-b4fd-0c5ec24d0be2",
   "metadata": {},
   "source": [
    "### Config / Column Names ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "7f1d274a-6a98-4347-90da-656668de7b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIGITS = string.digits\n",
    "LOWER = string.ascii_lowercase\n",
    "UPPER = string.ascii_uppercase\n",
    "DEFAULT_PUNCT = \"!#$%&()*+,-.:;<=>?@[]^_{|}~\"\n",
    "CHAR_TYPE_CHOICES = [\"digit\", \"lower\", \"upper\", \"punct\"]\n",
    "#defines UserRecord and GarbleTokenMapper for obfuscation.\n",
    "\n",
    "@dataclass #shortcut class go brrr\n",
    "class UserRecord:\n",
    "    token: str\n",
    "    count: int\n",
    "    valid: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "a8e9059c-4f51-4314-bbf4-b261fdc6aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GarbleTokenMapper:\n",
    "    \"\"\"\n",
    "    Sequential (non-random) token mapper.\n",
    "\n",
    "    - Users:  tokens like \"UR1\", \"UR2\", ...\n",
    "    - IPs:    tokens like \"IP1\", \"IP2\", ...\n",
    "\n",
    "    Keeps the same public API as the previous GarbleTokenMapper:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prefix: str = \"\",\n",
    "        start: int = 1,\n",
    "        # legacy args kept for drop-in compatibility; ignored\n",
    "        token_len: int = 8,\n",
    "        allow_punctuation: bool = False,\n",
    "        punct_chars: Optional[str] = None,\n",
    "    ):\n",
    "        self.prefix = str(prefix or \"\")\n",
    "        self.start = int(start)\n",
    "        # original -> UserRecord(token, count, valid)\n",
    "        self._by_orig: Dict[str, UserRecord] = {}\n",
    "        # token   -> original\n",
    "        self._token_to_orig: Dict[str, str] = {}\n",
    "        # issued tokens (not strictly needed for sequential, kept for parity)\n",
    "        self._seen_tokens = set()\n",
    "        # counter points to the LAST issued number (so next is _counter + 1)\n",
    "        self._counter = self.start - 1\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_trailing_int(s: str) -> Optional[int]:\n",
    "        m = re.search(r\"(\\d+)$\", str(s))\n",
    "        return int(m.group(1)) if m else None\n",
    "\n",
    "    def _next_token(self) -> str:\n",
    "        self._counter += 1\n",
    "        return f\"{self.prefix}{self._counter}\"\n",
    "\n",
    "    def add(self, original: str, valid: bool = True) -> str:\n",
    "        key = str(original)\n",
    "        if key in self._by_orig:\n",
    "            rec = self._by_orig[key]\n",
    "            rec.count += 1\n",
    "            return rec.token\n",
    "\n",
    "        token = self._next_token()\n",
    "        self._seen_tokens.add(token)\n",
    "        rec = UserRecord(token=token, count=1, valid=bool(valid))\n",
    "        self._by_orig[key] = rec\n",
    "        self._token_to_orig[token] = key\n",
    "        return token\n",
    "\n",
    "    def original_from_token(self, token: str) -> Optional[str]:\n",
    "        return self._token_to_orig.get(str(token))\n",
    "\n",
    "    def record_from_token(self, token: str) -> Optional[UserRecord]:\n",
    "        orig = self._token_to_orig.get(str(token))\n",
    "        return self._by_orig.get(orig) if orig is not None else None\n",
    "\n",
    "    def export_to_json(self, filepath: str) -> None:\n",
    "        entries = []\n",
    "        for orig, rec in self._by_orig.items():\n",
    "            e = asdict(rec)\n",
    "            e[\"original\"] = orig\n",
    "            entries.append(e)\n",
    "        state = {\n",
    "            \"entries\": entries,\n",
    "            \"config\": {\"prefix\": self.prefix, \"start\": self.start, \"counter\": self._counter},\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(filepath) or \".\", exist_ok=True)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(state, f, indent=2)\n",
    "\n",
    "    def load_from_json(self, filepath: str) -> None:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        entries = data.get(\"entries\", [])\n",
    "        cfg = data.get(\"config\", {})\n",
    "\n",
    "        # reset\n",
    "        self._by_orig.clear()\n",
    "        self._token_to_orig.clear()\n",
    "        self._seen_tokens.clear()\n",
    "\n",
    "        # keep existing prefix/start unless provided in file\n",
    "        self.prefix = str(cfg.get(\"prefix\", self.prefix))\n",
    "        self.start = int(cfg.get(\"start\", self.start))\n",
    "\n",
    "        max_num = self.start - 1\n",
    "        for e in entries:\n",
    "            orig = str(e[\"original\"])\n",
    "            token = str(e[\"token\"])\n",
    "            count = int(e.get(\"count\", 0))\n",
    "            valid = bool(e.get(\"valid\", True))\n",
    "            rec = UserRecord(token=token, count=count, valid=valid)\n",
    "            self._by_orig[orig] = rec\n",
    "            self._token_to_orig[token] = orig\n",
    "            self._seen_tokens.add(token)\n",
    "            n = self._extract_trailing_int(token)\n",
    "            if n is not None:\n",
    "                max_num = max(max_num, n)\n",
    "\n",
    "        # resume counting AFTER the largest seen number\n",
    "        self._counter = int(cfg.get(\"counter\", max_num))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5a0b5-d1aa-4158-b1ce-887313a16711",
   "metadata": {},
   "source": [
    "### Token Mapper (Part 2) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "6e2670f8-efef-4c42-9bc3-ba724f48830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_json(self, filepath: str):\n",
    "        entries = []\n",
    "        for orig, rec in self._by_orig.items():\n",
    "            e = asdict(rec)\n",
    "            e[\"original\"] = orig\n",
    "            entries.append(e)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"entries\": entries}, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "59adbae2-619a-4b99-8490-e18148730f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_json(self, filepath: str):\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        entries = data.get(\"entries\", [])\n",
    "\n",
    "        # clear current\n",
    "        self._by_orig.clear()\n",
    "        self._token_to_orig.clear()\n",
    "        self._seen_tokens.clear()\n",
    "\n",
    "        for e in entries:\n",
    "            orig = e[\"original\"]\n",
    "            token = e[\"token\"]\n",
    "            count = int(e.get(\"count\", 0))\n",
    "            valid = bool(e.get(\"valid\", True))\n",
    "            rec = UserRecord(token=token, count=count, valid=valid)\n",
    "            self._by_orig[orig] = rec\n",
    "            self._token_to_orig[token] = orig\n",
    "            self._seen_tokens.add(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a8c648-9717-40ac-bbeb-ae690287a0c7",
   "metadata": {},
   "source": [
    "#### Json \"export\" ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "507e689b-14ea-42bf-b467-598b7a3b05f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ipv4_re = re.compile(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\")\n",
    "\n",
    "def is_valid_user(u) -> bool:\n",
    "    if pd.isna(u):\n",
    "        return False\n",
    "    s = str(u).strip()\n",
    "    return len(s) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "5d72af62-7bc8-4974-8e5d-acbc3c16dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_ipv4(ip) -> bool:\n",
    "    if pd.isna(ip):\n",
    "        return False\n",
    "    s = str(ip).strip()\n",
    "    if not _ipv4_re.match(s):\n",
    "        return False\n",
    "    try:\n",
    "        parts = [int(p) for p in s.split(\".\")]\n",
    "    except ValueError:\n",
    "        return False\n",
    "    return all(0 <= p <= 255 for p in parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "0f978ea1-e247-4cbe-8264-7641b0d59f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_jagged_array(ob_dict: Dict[str, Dict[str, object]]) -> List[List[object]]:\n",
    "    \"\"\"\n",
    "    Anonymized jagged array builder:\n",
    "      input: { original: {\"id\": token, \"count\": int, \"valid\": bool}, ... }\n",
    "      output: [[token, count, valid], ...]   # NO original included\n",
    "    \"\"\"\n",
    "    return [[data[\"id\"], data[\"count\"], data[\"valid\"]]\n",
    "            for _, data in ob_dict.items()] #throwaway with keys to get values in tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "dafb811d-8617-48ea-87d1-927883490d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_json(obj, path: str):\n",
    "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f450552d-f357-4a6a-807e-867ffa1e01fb",
   "metadata": {},
   "source": [
    "#### Helpers/Secondary ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "14679a93-236b-4535-ac67-71c4c958acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframe(data_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads all *.parquet in data_dir with DuckDB and returns a DataFrame.\n",
    "    Only selects columns that actually exist across the files.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(data_dir):\n",
    "        raise FileNotFoundError(f\"DATA_DIR does not exist or is not a directory: {data_dir}\")\n",
    "\n",
    "    pattern = f\"{data_dir}/*.parquet\"\n",
    "\n",
    "    # Desired columns (with the corrected \"Environment\" name)\n",
    "    desired_cols = [\n",
    "        \"User\",\n",
    "        \"RequestMemory\",\n",
    "        \"CumulativeSlotTime\",\n",
    "        \"JobsubClientIpAddress\",\n",
    "        \"MATCH_EXP_JOB_Site\",\n",
    "        \"DAG_NodesFailed\",\n",
    "        \"NumJobCompletions\",\n",
    "        \"NumJobStarts\",\n",
    "        \"Cmd\",\n",
    "        \"Environment\",\n",
    "    ]\n",
    "\n",
    "    # Peek schema (LIMIT 0) to learn available columns across all files\n",
    "    # NOTE: read_parquet() properly handles glob patterns.\n",
    "    schema_df = duckdb.sql(f\"SELECT * FROM read_parquet('{pattern}') LIMIT 0\").df()\n",
    "    available = set(schema_df.columns)\n",
    "\n",
    "    present = [c for c in desired_cols if c in available]\n",
    "    missing = [c for c in desired_cols if c not in available]\n",
    "    if missing:\n",
    "        print(f\"[load_dataframe] Warning: missing columns not found in any file: {missing}\")\n",
    "\n",
    "    if not present:\n",
    "        raise RuntimeError(\"None of the desired columns are present in the parquet files.\")\n",
    "\n",
    "    # Quote identifiers to preserve any case-sensitive names\n",
    "    q = \", \".join([f'\"{c}\"' for c in present])\n",
    "    query = f\"SELECT {q} FROM read_parquet('{pattern}')\"\n",
    "    return duckdb.sql(query).df()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc69ebb2-e7a7-4798-a8e2-0408e815bac6",
   "metadata": {},
   "source": [
    "#### Loading Data ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "480a20d2-3da8-4b46-b653-92ecb3d02ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_obfuscations(\n",
    "    df: pd.DataFrame,\n",
    "    user_col: str = USER_COL,\n",
    "    ip_col: str = IP_COL,\n",
    ") -> Tuple[Dict[str, Dict[str, object]], Dict[str, Dict[str, object]], GarbleTokenMapper, GarbleTokenMapper]:\n",
    "    \"\"\"\n",
    "    Build two dicts:\n",
    "      users_dict = { original_user: {\"id\": token, \"count\": n, \"valid\": bool}, ... }\n",
    "      ips_dict   = { original_ip:   {\"id\": token, \"count\": n, \"valid\": bool}, ... }\n",
    "    Returns also the underlying mappers (useful if you want to export the maps).\n",
    "\n",
    "    Tokens are sequential:\n",
    "      - Users: UR1, UR2, ...\n",
    "      - IPs:   IP1, IP2, ...\n",
    "    \"\"\"\n",
    "    user_mapper = GarbleTokenMapper(prefix=\"UR\", start=1)\n",
    "    ip_mapper   = GarbleTokenMapper(prefix=\"IP\", start=1)\n",
    "\n",
    "    # iterate rows to add and count\n",
    "    for _, row in df.iterrows():\n",
    "        u = row.get(user_col)\n",
    "        ip = row.get(ip_col)\n",
    "\n",
    "        user_mapper.add(str(u), valid=is_valid_user(u))\n",
    "        ip_mapper.add(str(ip), valid=is_valid_ipv4(ip))\n",
    "\n",
    "    users_dict = {\n",
    "        orig: {\"id\": rec.token, \"count\": rec.count, \"valid\": rec.valid}\n",
    "        for orig, rec in user_mapper._by_orig.items()\n",
    "    }\n",
    "    ips_dict = {\n",
    "        orig: {\"id\": rec.token, \"count\": rec.count, \"valid\": rec.valid}\n",
    "        for orig, rec in ip_mapper._by_orig.items()\n",
    "    }\n",
    "    return users_dict, ips_dict, user_mapper, ip_mapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a422fc2-fdab-403d-8efb-f4fc92b5ee16",
   "metadata": {},
   "source": [
    "#### Transform ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "c6856717-5d06-44e1-a82b-cd8cde84ed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_output_json(\n",
    "    df: pd.DataFrame,\n",
    "    users_dict: Dict[str, Dict[str, object]],\n",
    "    ips_dict: Dict[str, Dict[str, object]],\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build the final JSON payload that includes jagged arrays and minimal metadata.\n",
    "    \"\"\"\n",
    "    users_jagged = to_jagged_array(users_dict)\n",
    "    ips_jagged   = to_jagged_array(ips_dict)\n",
    "\n",
    "    payload = {\n",
    "        \"users\": users_jagged,   # [[original, token, count, valid], ...]\n",
    "        \"ips\":   ips_jagged,     # [[original, token, count, valid], ...]\n",
    "        \"meta\": {\n",
    "            \"total_rows\": int(len(df)),\n",
    "            \"distinct_users\": int(len(users_dict)),\n",
    "            \"distinct_ips\": int(len(ips_dict)),\n",
    "        },\n",
    "    }\n",
    "    return json.dumps(payload, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f536cb23-30ed-4dfc-b8a5-131c24a5597f",
   "metadata": {},
   "source": [
    "#### Generic User json (Below) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "08a5b8be-bf92-40cb-9263-f314ea019d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_summary_payload(\n",
    "    df: pd.DataFrame,\n",
    "    users_dict: Dict[str, Dict[str, object]],\n",
    "    ips_dict: Dict[str, Dict[str, object]],\n",
    "    user_col: str = USER_COL,\n",
    "    ip_col: str = IP_COL,\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Build the summary payload and include per-user/IP correlation records:\n",
    "      - users: [[token, count, valid], ...]               (no originals)\n",
    "      - ips:   [[token, count, valid], ...]               (no originals)\n",
    "      - user_ip_correlations: [\n",
    "            [original_user, user_token, original_ip, ip_token, frequency, user_valid],\n",
    "            ...\n",
    "        ]\n",
    "    \"\"\"\n",
    "\n",
    "    # Anonymized jagged arrays (no originals)\n",
    "    users_jagged_anon = [[d[\"id\"], d[\"count\"], d[\"valid\"]] for d in users_dict.values()]\n",
    "    ips_jagged_anon   = [[d[\"id\"], d[\"count\"], d[\"valid\"]] for d in ips_dict.values()]\n",
    "\n",
    "    # Compute, for each user, the most frequent (mode) IP they used\n",
    "    def _pick_mode_ip(series: pd.Series) -> Optional[str]:\n",
    "        ser = series.dropna().astype(str)\n",
    "        if ser.empty:\n",
    "            return None\n",
    "        return ser.value_counts().idxmax()\n",
    "\n",
    "    tmp = df[[user_col, ip_col]].copy()\n",
    "    tmp[user_col] = tmp[user_col].astype(str)\n",
    "    top_ip_for_user = tmp.groupby(user_col)[ip_col].apply(_pick_mode_ip)\n",
    "\n",
    "    # Build correlated records\n",
    "    user_ip_correlations = []\n",
    "    for orig_user, udata in users_dict.items():\n",
    "        key_user = str(orig_user)\n",
    "        ip_orig = top_ip_for_user.get(key_user, None)\n",
    "        ip_token = ips_dict.get(ip_orig, {}).get(\"id\") if ip_orig is not None else None\n",
    "        user_ip_correlations.append([\n",
    "            key_user,                 # original user\n",
    "            udata[\"id\"],              # garbled user\n",
    "            ip_orig,                  # user's (mode) IP original\n",
    "            ip_token,                 # garbled IP\n",
    "            int(udata[\"count\"]),      # frequency (user count)\n",
    "            bool(udata[\"valid\"]),     # user validity\n",
    "        ])\n",
    "\n",
    "    return {\n",
    "        \"users\": users_jagged_anon,\n",
    "        \"ips\": ips_jagged_anon,\n",
    "        \"user_ip_correlations\": user_ip_correlations,\n",
    "        \"meta\": {\n",
    "            \"total_rows\": int(len(df)),\n",
    "            \"distinct_users\": int(len(users_dict)),\n",
    "            \"distinct_ips\": int(len(ips_dict)),\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "06a4b728-b376-4b69-aeb5-21948dd350c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _s(x):\n",
    "    #in case of NaN/none\n",
    "    return \"\" if pd.isna(x) else str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "320eb95c-74f7-4eab-b87e-1ab6f13ff259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_env(env_raw):\n",
    "    \"\"\"\n",
    "    Parses Environment column in sorted list by [key, value].\n",
    "    Accepts:\n",
    "      -Jsons, strings, key valu\n",
    "    creates [env, string]\n",
    "    \"\"\"\n",
    "    def _sort_key(pair):\n",
    "        #Case sens.\n",
    "        return pair[0].lower()\n",
    "\n",
    "    s = _s(env_raw).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "\n",
    "    # Json\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, dict):\n",
    "            pairs = []\n",
    "            for k, v in obj.items():\n",
    "                val = \"\" if v is None else str(v)\n",
    "                pairs.append((str(k), val))\n",
    "            return sorted(pairs, key=_sort_key)\n",
    "\n",
    "        # Json arrays of KEY=VAL or dicts.\n",
    "        if isinstance(obj, list):\n",
    "            pairs = []\n",
    "            for item in obj:\n",
    "                if isinstance(item, dict):\n",
    "                    for k, v in item.items():\n",
    "                        val = \"\" if v is None else str(v)\n",
    "                        pairs.append((str(k), val))\n",
    "                elif isinstance(item, str) and \"=\" in item:\n",
    "                    k, v = item.split(\"=\", 1)\n",
    "                    pairs.append((k.strip(), v.strip()))\n",
    "                else:\n",
    "                    pairs.append((\"ITEM\", str(item)))\n",
    "            return sorted(pairs, key=_sort_key)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Dict.\n",
    "    if (s.startswith(\"{\") and s.endswith(\"}\")) or (s.startswith(\"dict(\") and s.endswith(\")\")):\n",
    "        try:\n",
    "            s_jsonish = s.replace(\"'\", \"\\\"\")\n",
    "            obj = json.loads(s_jsonish)\n",
    "            if isinstance(obj, dict):\n",
    "                pairs = []\n",
    "                for k, v in obj.items():\n",
    "                    val = \"\" if v is None else str(v)\n",
    "                    pairs.append((str(k), val))\n",
    "                return sorted(pairs, key=_sort_key)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # KEY=VAL pars.\n",
    "    candidates = []\n",
    "    for delim in [\";\", \",\", \"\\n\"]:\n",
    "        if delim in s:\n",
    "            candidates = [p for p in s.split(delim)]\n",
    "            break\n",
    "    if not candidates:\n",
    "        # space-separated tokens; keep tokens that look like KEY=VAL\n",
    "        candidates = s.split()\n",
    "\n",
    "    pairs = []\n",
    "    for token in candidates:\n",
    "        token = token.strip()\n",
    "        if not token:\n",
    "            continue\n",
    "        if \"=\" in token:\n",
    "            k, v = token.split(\"=\", 1)\n",
    "            pairs.append((k.strip(), v.strip()))\n",
    "    if pairs:\n",
    "        return sorted(pairs, key=_sort_key)\n",
    "\n",
    "    # Fallback\n",
    "    return [(\"ENV\", s)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "70f1731d-7518-47d2-b997-f5ba764af31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _wrap_block(text, width):\n",
    "    return fill(_s(text), width=width, replace_whitespace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "1e35aa5e-596d-4c06-ac71-058a1d3d6d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_env_block(env_pairs, width, indent_spaces=2):\n",
    "    if not env_pairs:\n",
    "        return \"  (none)\"\n",
    "    lines = []\n",
    "    for k, v in env_pairs:\n",
    "        # \"KEY=VALUE\" with wrapping of thevalue\n",
    "        if v:\n",
    "            wrapped_v = fill(v, width=width - (len(k) + 1 + indent_spaces),\n",
    "                             subsequent_indent=\" \" * (len(k) + 1))\n",
    "            lines.append(f\"{k}={wrapped_v}\")\n",
    "        else:\n",
    "            lines.append(f\"{k}=\")\n",
    "    return indent(\"\\n\".join(lines), \" \" * indent_spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3143de-7264-48ec-92f4-980b82bf0d63",
   "metadata": {},
   "source": [
    "### Helpers ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "999a7f19-7d0a-48cf-bef9-6d374da2fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_cmd_env_report(\n",
    "    df: pd.DataFrame,\n",
    "    out_path: str | Path,\n",
    "    *,\n",
    "    group_by: str | None = None,   # e.g., \"User\" or None for flat list\n",
    "    human_wrap: int = HUMAN_WRAP,\n",
    "    include_meta: bool = True,\n",
    "    meta_cols: tuple[str, ...] = (\"User\", \"JobsubClientIpAddress\",\n",
    "                                  \"CumulativeSlotTime\", \"DAG_NodesFailed\",\n",
    "                                  \"NumJobStarts\", \"NumJobCompletions\"),\n",
    "    cmd_col: str = \"Cmd\",\n",
    "    env_col: str = \"Environment\",\n",
    ") -> Path:\n",
    "\n",
    "    p = Path(out_path)\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Ensure required cols exist (silently skip if missing)\n",
    "    cols_needed = set([cmd_col, env_col]) | (set(meta_cols) if include_meta else set())\n",
    "    missing = [c for c in cols_needed if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"[write_cmd_env_report] Warning: missing columns {missing}; proceeding with what exists.\")\n",
    "\n",
    "    def format_one(idx, row) -> str:\n",
    "        parts = []\n",
    "\n",
    "        # Header line\n",
    "        header = f\"— Job #{idx} —\"\n",
    "        parts.append(header)\n",
    "\n",
    "        # Meta block (compact, only present columns)\n",
    "        if include_meta:\n",
    "            for c in meta_cols:\n",
    "                if c in row.index:\n",
    "                    val = _s(row[c])\n",
    "                    if c == cmd_col or c == env_col:\n",
    "                        # don't duplicate\n",
    "                        continue\n",
    "                    # keep meta short; wrap very long fields\n",
    "                    if len(val) > human_wrap:\n",
    "                        val = _wrap_block(val, human_wrap)\n",
    "                    parts.append(f\"{c}: {val}\")\n",
    "\n",
    "        # Cmd\n",
    "        if cmd_col in row.index:\n",
    "            parts.append(\"Cmd:\")\n",
    "            parts.append(indent(_wrap_block(row[cmd_col], human_wrap), \"  \"))\n",
    "\n",
    "        # Environment\n",
    "        if env_col in row.index:\n",
    "            parts.append(\"Environment:\")\n",
    "            env_pairs = _parse_env(row[env_col])\n",
    "            parts.append(_format_env_block(env_pairs, width=human_wrap, indent_spaces=2))\n",
    "\n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "    lines_out = []\n",
    "\n",
    "    title = \"Job Command & Environment Report\"\n",
    "    meta_summary = f\"Total rows: {len(df)}\"\n",
    "    lines_out += [title, meta_summary, \"=\" * max(28, len(title)), \"\"]\n",
    "\n",
    "    if group_by and group_by in df.columns:\n",
    "        for gval, gdf in df.groupby(group_by, dropna=False):\n",
    "            header = f\"## {group_by}: {_s(gval)}  (jobs: {len(gdf)})\"\n",
    "            lines_out += [header, \"-\" * len(header)]\n",
    "            for i, (_, row) in enumerate(gdf.iterrows(), start=1):\n",
    "                lines_out.append(format_one(i, row))\n",
    "                lines_out.append(\"\")  # blank line between jobs\n",
    "            lines_out.append(\"\")      # blank line between groups\n",
    "    else:\n",
    "        for i, (_, row) in enumerate(df.iterrows(), start=1):\n",
    "            lines_out.append(format_one(i, row))\n",
    "            lines_out.append(\"\")\n",
    "\n",
    "    txt = \"\\n\".join(lines_out).rstrip() + \"\\n\"\n",
    "    p.write_text(txt, encoding=\"utf-8\")\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d591f6c3-e44e-40ff-a5b0-f989fe79bd6e",
   "metadata": {},
   "source": [
    "### main writer ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2332e6-02aa-4bd9-9822-53486e781eab",
   "metadata": {},
   "source": [
    "## cmd and envior. ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ae540-7274-42da-84c3-5c8da268f421",
   "metadata": {},
   "source": [
    "#### Failed User json (Below) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "314574ec-e2dc-42f9-821d-7124a904edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def failed_users_payload(\n",
    "    df: pd.DataFrame,\n",
    "    user_mapper: GarbleTokenMapper,\n",
    "    user_col: str = USER_COL,\n",
    "    starts_col: str = NUM_STARTS_COL,\n",
    "    completions_col: str = NUM_COMPLETIONS_COL,\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Create a payload listing JUST users with failed jobs, WITHOUT originals.\n",
    "    Each record: { \"token\": <str>, \"failure_count\": <int>, \"valid\": <bool> }\n",
    "    \"\"\"\n",
    "    if not {user_col, starts_col, completions_col} <= set(df.columns):\n",
    "        return {\n",
    "            \"failed_users\": [],\n",
    "            \"meta\": {\n",
    "                \"distinct_failed_users\": 0,\n",
    "                \"total_failure_rows\": 0,\n",
    "                \"note\": \"Required columns missing; cannot compute failed users.\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    mask_fail = (df[completions_col].astype(\"int\") == 0) & (df[starts_col] > 0)\n",
    "    failed_df = df.loc[mask_fail, [user_col]]\n",
    "\n",
    "    # count failure rows per user\n",
    "    fail_counts = failed_df.groupby(user_col)[user_col].count().rename(\"failure_count\")\n",
    "\n",
    "    records = []\n",
    "    total_failure_rows = int(fail_counts.sum()) if not fail_counts.empty else 0\n",
    "\n",
    "    for orig_user, fcount in fail_counts.items():\n",
    "        token = user_mapper.add(str(orig_user), valid=is_valid_user(orig_user))\n",
    "        records.append({\n",
    "            \"token\": token,\n",
    "            \"failure_count\": int(fcount),\n",
    "            \"valid\": is_valid_user(orig_user),\n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"failed_users\": records,\n",
    "        \"meta\": {\n",
    "            \"distinct_failed_users\": int(len(records)),\n",
    "            \"total_failure_rows\": total_failure_rows,\n",
    "        },\n",
    "    }\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "e96fb69a-e7e6-48dd-a58b-6faf4960a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def site_jobs_payload(\n",
    "    df: pd.DataFrame,\n",
    "    site: str = \"FermiGrid\",\n",
    "    *,\n",
    "    site_col: str = \"MATCH_EXP_JOB_Site\",\n",
    "    case_insensitive: bool = False,\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Payload of jobs given site\n",
    "    \"\"\"\n",
    "    # Check required column\n",
    "    if site_col not in df.columns:\n",
    "        return {\n",
    "            \"jobs_at_site\": [],\n",
    "            \"meta\": {\n",
    "                \"site\": site,\n",
    "                \"total_jobs_at_site\": 0,\n",
    "                \"note\": f\"Missing column: {site_col}\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    # Build mask to select the site\n",
    "    site_series = df[site_col].astype(str)\n",
    "    if case_insensitive:\n",
    "        mask = site_series.str.strip().str.casefold() == str(site).strip().casefold()\n",
    "    else:\n",
    "        mask = site_series.str.strip() == str(site).strip()\n",
    "\n",
    "    # Filter dataframe\n",
    "    df_site = df.loc[mask].copy()\n",
    "\n",
    "    # Convert to list of dictionaries for json for readability\n",
    "    job_records = df_site.to_dict(orient=\"records\")\n",
    "\n",
    "    payload = {\n",
    "        \"jobs_at_site\": job_records,\n",
    "        \"meta\": {\n",
    "            \"site\": site,\n",
    "            \"total_jobs_at_site\": int(len(df_site)),\n",
    "            \"columns_included\": list(df_site.columns),\n",
    "        },\n",
    "    }\n",
    "    return payload\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e5a48d-e8a0-4b86-8ed2-21c113e39bef",
   "metadata": {},
   "source": [
    "#### sites JSON ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4339a7a2-11fa-4f14-b42e-f7afc79e5eb3",
   "metadata": {},
   "source": [
    "#### Full output ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "d28d6d55-9ca7-40e9-bdaf-3bdcd06cac53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Small samples ===\n",
      "users_dict sample: {\n",
      "  \"uboonepro@fnal.gov\": {\n",
      "    \"id\": \"UR1\",\n",
      "    \"count\": 99239,\n",
      "    \"valid\": true\n",
      "  },\n",
      "  \"icaruspro@fnal.gov\": {\n",
      "    \"id\": \"UR2\",\n",
      "    \"count\": 47080,\n",
      "    \"valid\": true\n",
      "  },\n",
      "  \"gputnam@fnal.gov\": {\n",
      "    \"id\": \"UR3\",\n",
      "    \"count\": 12693,\n",
      "    \"valid\": true\n",
      "  }\n",
      "}\n",
      "ips_dict sample  : {\n",
      "  \"131.225.240.146\": {\n",
      "    \"id\": \"IP1\",\n",
      "    \"count\": 86225,\n",
      "    \"valid\": true\n",
      "  },\n",
      "  \"131.225.240.90\": {\n",
      "    \"id\": \"IP2\",\n",
      "    \"count\": 47080,\n",
      "    \"valid\": true\n",
      "  },\n",
      "  \"131.225.240.140\": {\n",
      "    \"id\": \"IP3\",\n",
      "    \"count\": 12693,\n",
      "    \"valid\": true\n",
      "  }\n",
      "}\n",
      "\n",
      "summary.json preview:\n",
      " {\n",
      "  \"users\": [\n",
      "    [\n",
      "      \"UR1\",\n",
      "      99239,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR2\",\n",
      "      47080,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR3\",\n",
      "      12693,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR4\",\n",
      "      3652,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR5\",\n",
      "      15298,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR6\",\n",
      "      621,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR7\",\n",
      "      28340,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR8\",\n",
      "      1508,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR9\",\n",
      "      329,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR10\",\n",
      "      6867,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR11\",\n",
      "      417,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR12\",\n",
      "      105,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR13\",\n",
      "      52,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR14\",\n",
      "      42,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR15\",\n",
      "      108,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR16\",\n",
      "      17,\n",
      "      true\n",
      "    ],\n",
      " ...\n",
      "\n",
      "Job failure fraction %: 9.581%, job failure abs number: 24400\n",
      "Wrote: Output/cmd_env_report.txt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    df = load_dataframe(DATA_DIR)\n",
    "\n",
    "    # Build obfuscations\n",
    "    users_dict, ips_dict, user_mapper, ip_mapper = build_obfuscations(\n",
    "        df, user_col=USER_COL, ip_col=IP_COL\n",
    "    )\n",
    "\n",
    "    # --- Create payloads / JSON strings ---\n",
    "    summary_obj = make_summary_payload(df, users_dict, ips_dict)\n",
    "    summary_json = json.dumps(summary_obj, indent=2)\n",
    "\n",
    "    # Explicit JSON for the jagged arrays (as requested)\n",
    "    users_jagged = to_jagged_array(users_dict)\n",
    "    ips_jagged   = to_jagged_array(ips_dict)\n",
    "    users_jagged_obj = {\n",
    "        \"users\": users_jagged,\n",
    "        \"meta\": {\n",
    "            \"distinct_users\": int(len(users_dict)),\n",
    "            \"total_rows\": int(len(df)),\n",
    "        },\n",
    "    }\n",
    "    ips_jagged_obj = {\n",
    "        \"ips\": ips_jagged,\n",
    "        \"meta\": {\n",
    "            \"distinct_ips\": int(len(ips_dict)),\n",
    "            \"total_rows\": int(len(df)),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Failed users payload (JUST the users with failed jobs)\n",
    "    failed_obj = failed_users_payload(\n",
    "        df,\n",
    "        user_mapper=user_mapper,\n",
    "        user_col=USER_COL,\n",
    "        starts_col=NUM_STARTS_COL,\n",
    "        completions_col=NUM_COMPLETIONS_COL,\n",
    "    )\n",
    "\n",
    "    # --- Write files ---\n",
    "    summary_path = os.path.join(OUTPUT_DIR, \"summary.json\")\n",
    "    users_jagged_path = os.path.join(OUTPUT_DIR, \"users_jagged.json\")\n",
    "    ips_jagged_path = os.path.join(OUTPUT_DIR, \"ips_jagged.json\")\n",
    "    failed_users_path = os.path.join(OUTPUT_DIR, \"failed_users.json\")\n",
    "    out_path = os.path.join(OUTPUT_DIR, \"jobs_at_FermiGrid.json\")\n",
    "\n",
    "    jobs_at_FermiGrid = site_jobs_payload(df, site=\"FermiGrid\")\n",
    "    dump_json(jobs_at_FermiGrid, out_path)\n",
    "\n",
    "    dump_json(summary_obj, summary_path)\n",
    "    dump_json(users_jagged_obj, users_jagged_path)\n",
    "    dump_json(ips_jagged_obj, ips_jagged_path)\n",
    "    dump_json(failed_obj, failed_users_path)\n",
    "    \n",
    "    # --- Console peeks ---\n",
    "    #print(\"\\n=== Wrote JSON files ===\")\n",
    "    #print(\"summary         :\", summary_path)\n",
    "    #print(\"users_jagged    :\", users_jagged_path)\n",
    "    #print(\"ips_jagged      :\", ips_jagged_path)\n",
    "    #print(\"failed_users    :\", failed_users_path)\n",
    "\n",
    "    print(\"\\n=== Small samples ===\")\n",
    "    print(\"users_dict sample:\", json.dumps(dict(list(users_dict.items())[:3]), indent=2))\n",
    "    print(\"ips_dict sample  :\", json.dumps(dict(list(ips_dict.items())[:3]), indent=2))\n",
    "    print(\"\\nsummary.json preview:\\n\", summary_json[:800], \"...\\n\")\n",
    "\n",
    "    # --- Failure stats (for context) ---\n",
    "    total_starts = int(df[NUM_STARTS_COL].sum()) if NUM_STARTS_COL in df else 0\n",
    "    total_completions = int(df[NUM_COMPLETIONS_COL].astype(\"int\").sum()) if NUM_COMPLETIONS_COL in df else 0\n",
    "    n_job_failures = total_starts - total_completions\n",
    "    job_failure_frac = (n_job_failures / total_starts) if total_starts else 0.0\n",
    "    print(f\"Job failure fraction %: {job_failure_frac:.3%}, job failure abs number: {n_job_failures}\") #Job failures = (Jobs Started - Jobs finished)/ Jobs Started\n",
    "\n",
    " #Text file stuff\n",
    "    out_txt = Path(OUTPUT_DIR) / \"cmd_env_report.txt\"\n",
    "    write_cmd_env_report(df, out_txt)\n",
    "    print(\"Wrote:\", out_txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d3efae-e9a1-4d22-8cb4-06375e6803bf",
   "metadata": {},
   "source": [
    "#### Main ####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
