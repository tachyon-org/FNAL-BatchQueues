{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7b80bbb-dd2a-4ed6-8f12-7627ad56a48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nathan/git/FNAL-BatchQueues/notebooks/CodeAttemptFNAL_SeqVer.py\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from Library import anon_fnal as fnal # importing my stuff!\n",
    "# set your filename if needed\n",
    "file_path = Path(\"CodeAttemptFNAL_SeqVer.py\").resolve()\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8abb1db0-f204-42ae-8532-e6f591fd6b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Library import anon_fnal\n",
    "import importlib\n",
    "fnal = importlib.reload(anon_fnal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f4f76f-e634-488f-88fa-09bbf36e7834",
   "metadata": {},
   "source": [
    "##### Note: Code split between two parts for me. First part is gathering data to put in a way to use, second part is transforming it. Also, headers are below code. ##### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1ca27c-51a0-419c-9235-22031ee4dfcb",
   "metadata": {},
   "source": [
    "## Methods ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f2d68-5037-416b-ad30-b653ebedbbe4",
   "metadata": {},
   "source": [
    "\n",
    "(Garble Methods)\n",
    "\n",
    "\n",
    "\n",
    "add — returns a stable token for an original and increments its count.\n",
    "\n",
    "original_from_token — reverse-maps a token back to the original string.\n",
    "\n",
    "record_from_token — fetches the UserRecord associated with a token.\n",
    "\n",
    "export_to_json — saves the mapper’s current state to disk.\n",
    "\n",
    "load_from_json — restores the mapper’s state from a saved JSON file.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(Helper Methods)\n",
    "is_valid_user — checks that a user value is non-null and non-empty.\n",
    "\n",
    "is_valid_ipv4 — validates IPv4 dotted-quad format and range.\n",
    "\n",
    "to_jagged_array — builds [[original, token, count, valid], ...] for non-anonymous use.\n",
    "\n",
    "dump_json — writes a Python object to a pretty-printed JSON file.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(Data Methods)\n",
    "load_dataframe — selects required columns from all Parquet files in DATA_DIR.\n",
    "\n",
    "build_obfuscations — iterates rows to create user/IP token maps with counts/validity.\n",
    "\n",
    "make_summary_payload — returns anonymized users/IPs jagged arrays plus meta.\n",
    "\n",
    "failed_users_payload — returns anonymized records for users with failed jobs plus meta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "825b86e0-dcf0-4f5c-8084-62db4becd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "from typing import Dict, Optional, List, Tuple #typing helper\n",
    "#transform data\n",
    "#data I/O\n",
    "\n",
    "\n",
    "#For readable texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cd0ac9a-fdec-490d-b524-24fd8ebe5740",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "OUTPUT_DIR = \"./Output\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4385f10c-4a5f-41b2-ba35-5fd7bac11178",
   "metadata": {},
   "source": [
    "### Import stuff ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a5e1be3-23dd-4057-9e89-158ea8cce1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(argv=None) -> argparse.Namespace:\n",
    "    #Wrapper around build_arg_parser() so code can just call parse_args().\n",
    "    parser = build_arg_parser()\n",
    "\n",
    "    # If argv is None, decide based on environment\n",
    "    if argv is None:\n",
    "        if \"ipykernel\" in sys.argv[0]:\n",
    "            argv = []\n",
    "        else:\n",
    "            argv = sys.argv[1:]\n",
    "\n",
    "    return parser.parse_args(argv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89f112ac-05cd-4bf5-a6be-15df6be0b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_arg_parser() -> argparse.ArgumentParser:\n",
    "    p = argparse.ArgumentParser(add_help=False)  # avoid clashing with ipykernel flags\n",
    "\n",
    "    # IO + basics\n",
    "    p.add_argument(\"--data-dir\", default=DATA_DIR, help=\"Directory containing parquet files\")\n",
    "    p.add_argument(\"--output-dir\", default=OUTPUT_DIR, help=\"Directory to write outputs\")\n",
    "\n",
    "    # import stuff for human_wrap here\n",
    "    p.add_argument(\"--wrap\", type=int, default=fnal.HUMAN_WRAP, help=\"Wrap width for text reports\")\n",
    "\n",
    "    # Columns – use library defaults\n",
    "    p.add_argument(\"--user-col\", default=fnal.USER_COL)\n",
    "    p.add_argument(\"--ip-col\", default=fnal.IP_COL)\n",
    "    p.add_argument(\"--site-col\", default=\"MATCH_EXP_JOB_Site\")\n",
    "    p.add_argument(\"--cmd-col\", default=\"Cmd\")\n",
    "    p.add_argument(\"--env-col\", default=\"Environment\")\n",
    "    p.add_argument(\"--starts-col\", default=fnal.NUM_STARTS_COL)\n",
    "    p.add_argument(\"--completions-col\", default=fnal.NUM_COMPLETIONS_COL)\n",
    "\n",
    "    # Site selection\n",
    "    p.add_argument(\"--site\", default=\"FermiGrid\", help=\"Requested site to export\")\n",
    "    p.add_argument(\"--case-insensitive\", action=\"store_true\", default=True,\n",
    "                   help=\"Case-insensitive site match (default ON)\")\n",
    "    p.add_argument(\"--case-sensitive\", dest=\"case_insensitive\", action=\"store_false\",\n",
    "                   help=\"Turn OFF case-insensitive site match\")\n",
    "    p.add_argument(\"--all-sites\", action=\"store_true\", help=\"Emit one JSON per site\")\n",
    "\n",
    "    # Garbling\n",
    "    p.add_argument(\"--garble\", action=\"store_true\", default=True, help=\"Turn ON garbling (default ON)\")\n",
    "    p.add_argument(\"--no-garble\", dest=\"garble\", action=\"store_false\", help=\"Turn OFF garbling\")\n",
    "    p.add_argument(\"--user-prefix\", default=\"UR_\", help=\"Prefix for user-handle tokens\")\n",
    "    p.add_argument(\"--exp-prefix\", default=\"EX_\", help=\"Prefix for experiment tokens\")\n",
    "    p.add_argument(\"--experiments\", default=\"uboone,icarus,pip2,nova,dune\",\n",
    "                   help=\"Comma-separated experiment keywords\")\n",
    "\n",
    "    # Report controls\n",
    "    p.add_argument(\"--report-file\", default=\"cmd_env_report.txt\", help=\"Output TXT report filename\")\n",
    "    p.add_argument(\"--report-group-by\", default=None, help=\"Optional column to group report by (e.g., User)\")\n",
    "    p.add_argument(\"--include-meta\", action=\"store_true\", default=True)\n",
    "    p.add_argument(\"--no-meta\", dest=\"include_meta\", action=\"store_false\")\n",
    "\n",
    "    p.add_argument(\"-h\", \"--help\", action=\"help\", help=\"Show this help message and exit\")\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d28d6d55-9ca7-40e9-bdaf-3bdcd06cac53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ./Output/jobs_at_FermiGrid.json  (valid=True, request=FermiGrid, canonical=FermiGrid)\n",
      "Wrote: Output/cmd_env_report.txt\n",
      "\n",
      "=== Small samples ===\n",
      "users_dict sample: {\n",
      "  \"uboonepro@fnal.gov\": {\n",
      "    \"id\": \"UR1\",\n",
      "    \"count\": 86007,\n",
      "    \"valid\": true\n",
      "  },\n",
      "  \"icaruspro@fnal.gov\": {\n",
      "    \"id\": \"UR2\",\n",
      "    \"count\": 40223,\n",
      "    \"valid\": true\n",
      "  },\n",
      "  \"gputnam@fnal.gov\": {\n",
      "    \"id\": \"UR3\",\n",
      "    \"count\": 9852,\n",
      "    \"valid\": true\n",
      "  }\n",
      "}\n",
      "ips_dict sample: {\n",
      "  \"131.225.240.146\": {\n",
      "    \"id\": \"IP1\",\n",
      "    \"count\": 74541,\n",
      "    \"valid\": true\n",
      "  },\n",
      "  \"131.225.240.90\": {\n",
      "    \"id\": \"IP2\",\n",
      "    \"count\": 40223,\n",
      "    \"valid\": true\n",
      "  },\n",
      "  \"131.225.240.140\": {\n",
      "    \"id\": \"IP3\",\n",
      "    \"count\": 9852,\n",
      "    \"valid\": true\n",
      "  }\n",
      "}\n",
      "\n",
      "summary.json preview:\n",
      " {\n",
      "  \"users\": [\n",
      "    [\n",
      "      \"UR1\",\n",
      "      86007,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR2\",\n",
      "      40223,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR3\",\n",
      "      9852,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR4\",\n",
      "      3034,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR5\",\n",
      "      10432,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR6\",\n",
      "      540,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR7\",\n",
      "      23522,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR8\",\n",
      "      1207,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR9\",\n",
      "      287,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR10\",\n",
      "      5340,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR11\",\n",
      "      294,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR12\",\n",
      "      91,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR13\",\n",
      "      36,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR14\",\n",
      "      36,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR15\",\n",
      "      90,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR16\",\n",
      "      13,\n",
      "      true\n",
      "    ],\n",
      "    ...\n",
      "\n",
      "Job failure fraction %: 8.316%, job failure abs number: 15614\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "\n",
    "    # Runtime overrides\n",
    "    DATA_DIR_RUNTIME = args.data_dir\n",
    "    OUTPUT_DIR_RUNTIME = args.output_dir\n",
    "    HUMAN_WRAP_RUNTIME = args.wrap\n",
    "\n",
    "    USER_COL_RUNTIME = args.user_col\n",
    "    IP_COL_RUNTIME = args.ip_col\n",
    "    SITE_COL_RUNTIME = args.site_col\n",
    "    CMD_COL_RUNTIME = args.cmd_col\n",
    "    ENV_COL_RUNTIME = args.env_col\n",
    "    STARTS_COL_RUNTIME = args.starts_col\n",
    "    COMPLETIONS_COL_RUNTIME = args.completions_col\n",
    "\n",
    "    # Override known experiments inside library module\n",
    "    fnal.KNOWN_EXPERIMENTS = set(\n",
    "        x.strip() for x in args.experiments.split(\",\") if x.strip()\n",
    "    )\n",
    "\n",
    "    # Ensure output dir exists\n",
    "    os.makedirs(OUTPUT_DIR_RUNTIME, exist_ok=True)\n",
    "\n",
    "    # Load data\n",
    "    df = fnal.load_dataframe(DATA_DIR_RUNTIME)\n",
    "\n",
    "    # Dump selected jobs\n",
    "    selected_out = Path(OUTPUT_DIR_RUNTIME) / \"selected_jobs.json\"\n",
    "    fnal.dump_selected_job_fields(df, selected_out)\n",
    "\n",
    "    # Build obfuscations\n",
    "    users_dict, ips_dict, user_mapper, ip_mapper = fnal.build_obfuscations(\n",
    "        df,\n",
    "        user_col=USER_COL_RUNTIME,\n",
    "        ip_col=IP_COL_RUNTIME\n",
    "    )\n",
    "\n",
    "    # Build summary payload\n",
    "    summary_obj = fnal.make_summary_payload(\n",
    "        df,\n",
    "        users_dict,\n",
    "        ips_dict,\n",
    "        user_col=USER_COL_RUNTIME,\n",
    "        ip_col=IP_COL_RUNTIME\n",
    "    )\n",
    "    summary_json = json.dumps(summary_obj, indent=2)\n",
    "\n",
    "    # Jagged array outputs\n",
    "    users_jagged = fnal.to_jagged_array(users_dict)\n",
    "    ips_jagged = fnal.to_jagged_array(ips_dict)\n",
    "\n",
    "    users_jagged_obj = {\n",
    "        \"users\": users_jagged,\n",
    "        \"meta\": {\n",
    "            \"distinct_users\": len(users_dict),\n",
    "            \"total_rows\": len(df),\n",
    "        },\n",
    "    }\n",
    "    ips_jagged_obj = {\n",
    "        \"ips\": ips_jagged,\n",
    "        \"meta\": {\n",
    "            \"distinct_ips\": len(ips_dict),\n",
    "            \"total_rows\": len(df),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Failed users\n",
    "    failed_obj = fnal.failed_users_payload(\n",
    "        df,\n",
    "        user_mapper=user_mapper,\n",
    "        user_col=USER_COL_RUNTIME,\n",
    "        starts_col=STARTS_COL_RUNTIME,\n",
    "        completions_col=COMPLETIONS_COL_RUNTIME,\n",
    "    )\n",
    "\n",
    "    # Write JSON outputs\n",
    "    fnal.dump_json(summary_obj, os.path.join(OUTPUT_DIR_RUNTIME, \"summary.json\"))\n",
    "    fnal.dump_json(users_jagged_obj, os.path.join(OUTPUT_DIR_RUNTIME, \"users_jagged.json\"))\n",
    "    fnal.dump_json(ips_jagged_obj, os.path.join(OUTPUT_DIR_RUNTIME, \"ips_jagged.json\"))\n",
    "    fnal.dump_json(failed_obj, os.path.join(OUTPUT_DIR_RUNTIME, \"failed_users.json\"))\n",
    "\n",
    "    # Helper to make safe filenames\n",
    "    def change_filename(name: str) -> str:\n",
    "        return re.sub(r\"[^A-Za-z0-9_.-]+\", \"_\", str(name))\n",
    "\n",
    "    # Write individual site payload(s)\n",
    "    def write_site_payload_for(site_req: str):\n",
    "        payload = fnal.site_jobs_payload(\n",
    "            df,\n",
    "            site_name=site_req,\n",
    "            site_col=SITE_COL_RUNTIME,\n",
    "            case_insensitive=args.case_insensitive,\n",
    "            garble=args.garble,\n",
    "            user_col=USER_COL_RUNTIME,\n",
    "            cmd_col=CMD_COL_RUNTIME,\n",
    "            env_col=ENV_COL_RUNTIME,\n",
    "        )\n",
    "        name_for_file = payload[\"meta\"][\"canonical_site\"] or site_req\n",
    "        out_fname = f\"jobs_at_{change_filename(name_for_file)}.json\"\n",
    "        out_path = os.path.join(OUTPUT_DIR_RUNTIME, out_fname)\n",
    "        fnal.dump_json(payload, out_path)\n",
    "\n",
    "        print(\n",
    "            f\"Wrote: {out_path}  \"\n",
    "            f\"(valid={payload['meta']['is_valid_site']}, \"\n",
    "            f\"request={site_req}, canonical={payload['meta']['canonical_site']})\"\n",
    "        )\n",
    "\n",
    "    # Emit site payloads\n",
    "    if args.all_sites and SITE_COL_RUNTIME in df.columns:\n",
    "        sites = (\n",
    "            df[SITE_COL_RUNTIME]\n",
    "            .dropna()\n",
    "            .astype(str)\n",
    "            .map(str.strip)\n",
    "            .unique()\n",
    "            .tolist()\n",
    "        )\n",
    "        for s in sorted(sites, key=str.casefold):\n",
    "            write_site_payload_for(s)\n",
    "    else:\n",
    "        write_site_payload_for(args.site)\n",
    "\n",
    "    # Write text report\n",
    "    report_path = Path(OUTPUT_DIR_RUNTIME) / args.report_file\n",
    "    fnal.write_cmd_env_report(\n",
    "        df,\n",
    "        report_path,\n",
    "        group_by=args.report_group_by,\n",
    "        human_wrap=HUMAN_WRAP_RUNTIME,\n",
    "        include_meta=args.include_meta,\n",
    "        meta_cols=(\n",
    "            \"User\",\n",
    "            \"JobsubClientIpAddress\",\n",
    "            \"CumulativeSlotTime\",\n",
    "            \"DAG_NodesFailed\",\n",
    "            \"NumJobStarts\",\n",
    "            \"NumJobCompletions\",\n",
    "        ),\n",
    "        cmd_col=CMD_COL_RUNTIME,\n",
    "        env_col=ENV_COL_RUNTIME,\n",
    "    )\n",
    "    print(\"Wrote:\", report_path)\n",
    "\n",
    "    # Console samples and stats\n",
    "    print(\"\\n=== Small samples ===\")\n",
    "    print(\"users_dict sample:\", json.dumps(dict(list(users_dict.items())[:3]), indent=2))\n",
    "    print(\"ips_dict sample:\", json.dumps(dict(list(ips_dict.items())[:3]), indent=2))\n",
    "\n",
    "    print(\"\\nsummary.json preview:\\n\", summary_json[:800], \"...\\n\")\n",
    "\n",
    "    total_starts = int(df[STARTS_COL_RUNTIME].sum()) if STARTS_COL_RUNTIME in df else 0\n",
    "    total_completions = (\n",
    "        int(df[COMPLETIONS_COL_RUNTIME].astype(\"int\").sum())\n",
    "        if COMPLETIONS_COL_RUNTIME in df\n",
    "        else 0\n",
    "    )\n",
    "    n_job_failures = total_starts - total_completions\n",
    "    job_failure_frac = (n_job_failures / total_starts) if total_starts else 0.0\n",
    "\n",
    "    print(\n",
    "        f\"Job failure fraction %: {job_failure_frac:.3%}, \"\n",
    "        f\"job failure abs number: {n_job_failures}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d3efae-e9a1-4d22-8cb4-06375e6803bf",
   "metadata": {},
   "source": [
    "#### Main ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0716d2-6826-407b-8416-c5ca05c38021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
