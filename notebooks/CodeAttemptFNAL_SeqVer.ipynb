{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7b80bbb-dd2a-4ed6-8f12-7627ad56a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f4f76f-e634-488f-88fa-09bbf36e7834",
   "metadata": {},
   "source": [
    "##### Note: Code split between two parts for me. First part is gathering data to put in a way to use, second part is transforming it. Also, headers are below code. ##### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1ca27c-51a0-419c-9235-22031ee4dfcb",
   "metadata": {},
   "source": [
    "## Methods ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f2d68-5037-416b-ad30-b653ebedbbe4",
   "metadata": {},
   "source": [
    "\n",
    "(Garble Methods)\n",
    "\n",
    "\n",
    "\n",
    "add — returns a stable token for an original and increments its count.\n",
    "\n",
    "original_from_token — reverse-maps a token back to the original string.\n",
    "\n",
    "record_from_token — fetches the UserRecord associated with a token.\n",
    "\n",
    "export_to_json — saves the mapper’s current state to disk.\n",
    "\n",
    "load_from_json — restores the mapper’s state from a saved JSON file.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(Helper Methods)\n",
    "is_valid_user — checks that a user value is non-null and non-empty.\n",
    "\n",
    "is_valid_ipv4 — validates IPv4 dotted-quad format and range.\n",
    "\n",
    "to_jagged_array — builds [[original, token, count, valid], ...] for non-anonymous use.\n",
    "\n",
    "dump_json — writes a Python object to a pretty-printed JSON file.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(Data Methods)\n",
    "load_dataframe — selects required columns from all Parquet files in DATA_DIR.\n",
    "\n",
    "build_obfuscations — iterates rows to create user/IP token maps with counts/validity.\n",
    "\n",
    "make_summary_payload — returns anonymized users/IPs jagged arrays plus meta.\n",
    "\n",
    "failed_users_payload — returns anonymized records for users with failed jobs plus meta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "825b86e0-dcf0-4f5c-8084-62db4becd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import secrets\n",
    "from dataclasses import dataclass, asdict #asdict for json\n",
    "from typing import Dict, Optional, List, Tuple #tuple for serializing\n",
    "#For transforming data into what we want.\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "#For reading data and putting it into something usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cd0ac9a-fdec-490d-b524-24fd8ebe5740",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "OUTPUT_DIR = \"./Output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4385f10c-4a5f-41b2-ba35-5fd7bac11178",
   "metadata": {},
   "source": [
    "### Import stuff ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06a924c1-dddc-4b77-8ca3-f6af01587dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_COL   = \"User\"\n",
    "IP_COL     = \"JobsubClientIpAddress\"\n",
    "FAILED_COL = \"DAG_NodesFailed\"  # “boolean-ish”\n",
    "NUM_STARTS_COL     = \"NumJobStarts\"\n",
    "NUM_COMPLETIONS_COL= \"NumJobCompletions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bb0771-4508-4180-b4fd-0c5ec24d0be2",
   "metadata": {},
   "source": [
    "### Config / Column Names ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f1d274a-6a98-4347-90da-656668de7b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIGITS = string.digits\n",
    "LOWER = string.ascii_lowercase\n",
    "UPPER = string.ascii_uppercase\n",
    "DEFAULT_PUNCT = \"!#$%&()*+,-.:;<=>?@[]^_{|}~\"\n",
    "CHAR_TYPE_CHOICES = [\"digit\", \"lower\", \"upper\", \"punct\"]\n",
    "#defines UserRecord and GarbleTokenMapper for obfuscation.\n",
    "\n",
    "@dataclass #shortcut class go brrr\n",
    "class UserRecord:\n",
    "    token: str\n",
    "    count: int\n",
    "    valid: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8e9059c-4f51-4314-bbf4-b261fdc6aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GarbleTokenMapper:\n",
    "    \"\"\"\n",
    "    Sequential (non-random) token mapper.\n",
    "\n",
    "    - Users:  tokens like \"UR1\", \"UR2\", ...\n",
    "    - IPs:    tokens like \"IP1\", \"IP2\", ...\n",
    "\n",
    "    Keeps the same public API as the previous GarbleTokenMapper:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prefix: str = \"\",\n",
    "        start: int = 1,\n",
    "        # legacy args kept for drop-in compatibility; ignored\n",
    "        token_len: int = 8,\n",
    "        allow_punctuation: bool = False,\n",
    "        punct_chars: Optional[str] = None,\n",
    "    ):\n",
    "        self.prefix = str(prefix or \"\")\n",
    "        self.start = int(start)\n",
    "        # original -> UserRecord(token, count, valid)\n",
    "        self._by_orig: Dict[str, UserRecord] = {}\n",
    "        # token   -> original\n",
    "        self._token_to_orig: Dict[str, str] = {}\n",
    "        # issued tokens (not strictly needed for sequential, kept for parity)\n",
    "        self._seen_tokens = set()\n",
    "        # counter points to the LAST issued number (so next is _counter + 1)\n",
    "        self._counter = self.start - 1\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_trailing_int(s: str) -> Optional[int]:\n",
    "        m = re.search(r\"(\\d+)$\", str(s))\n",
    "        return int(m.group(1)) if m else None\n",
    "\n",
    "    def _next_token(self) -> str:\n",
    "        self._counter += 1\n",
    "        return f\"{self.prefix}{self._counter}\"\n",
    "\n",
    "    def add(self, original: str, valid: bool = True) -> str:\n",
    "        key = str(original)\n",
    "        if key in self._by_orig:\n",
    "            rec = self._by_orig[key]\n",
    "            rec.count += 1\n",
    "            return rec.token\n",
    "\n",
    "        token = self._next_token()\n",
    "        self._seen_tokens.add(token)\n",
    "        rec = UserRecord(token=token, count=1, valid=bool(valid))\n",
    "        self._by_orig[key] = rec\n",
    "        self._token_to_orig[token] = key\n",
    "        return token\n",
    "\n",
    "    def original_from_token(self, token: str) -> Optional[str]:\n",
    "        return self._token_to_orig.get(str(token))\n",
    "\n",
    "    def record_from_token(self, token: str) -> Optional[UserRecord]:\n",
    "        orig = self._token_to_orig.get(str(token))\n",
    "        return self._by_orig.get(orig) if orig is not None else None\n",
    "\n",
    "    def export_to_json(self, filepath: str) -> None:\n",
    "        entries = []\n",
    "        for orig, rec in self._by_orig.items():\n",
    "            e = asdict(rec)\n",
    "            e[\"original\"] = orig\n",
    "            entries.append(e)\n",
    "        state = {\n",
    "            \"entries\": entries,\n",
    "            \"config\": {\"prefix\": self.prefix, \"start\": self.start, \"counter\": self._counter},\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(filepath) or \".\", exist_ok=True)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(state, f, indent=2)\n",
    "\n",
    "    def load_from_json(self, filepath: str) -> None:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        entries = data.get(\"entries\", [])\n",
    "        cfg = data.get(\"config\", {})\n",
    "\n",
    "        # reset\n",
    "        self._by_orig.clear()\n",
    "        self._token_to_orig.clear()\n",
    "        self._seen_tokens.clear()\n",
    "\n",
    "        # keep existing prefix/start unless provided in file\n",
    "        self.prefix = str(cfg.get(\"prefix\", self.prefix))\n",
    "        self.start = int(cfg.get(\"start\", self.start))\n",
    "\n",
    "        max_num = self.start - 1\n",
    "        for e in entries:\n",
    "            orig = str(e[\"original\"])\n",
    "            token = str(e[\"token\"])\n",
    "            count = int(e.get(\"count\", 0))\n",
    "            valid = bool(e.get(\"valid\", True))\n",
    "            rec = UserRecord(token=token, count=count, valid=valid)\n",
    "            self._by_orig[orig] = rec\n",
    "            self._token_to_orig[token] = orig\n",
    "            self._seen_tokens.add(token)\n",
    "            n = self._extract_trailing_int(token)\n",
    "            if n is not None:\n",
    "                max_num = max(max_num, n)\n",
    "\n",
    "        # resume counting AFTER the largest seen number\n",
    "        self._counter = int(cfg.get(\"counter\", max_num))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5a0b5-d1aa-4158-b1ce-887313a16711",
   "metadata": {},
   "source": [
    "### Token Mapper (Part 2) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e2670f8-efef-4c42-9bc3-ba724f48830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_json(self, filepath: str):\n",
    "        entries = []\n",
    "        for orig, rec in self._by_orig.items():\n",
    "            e = asdict(rec)\n",
    "            e[\"original\"] = orig\n",
    "            entries.append(e)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"entries\": entries}, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59adbae2-619a-4b99-8490-e18148730f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_json(self, filepath: str):\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        entries = data.get(\"entries\", [])\n",
    "\n",
    "        # clear current\n",
    "        self._by_orig.clear()\n",
    "        self._token_to_orig.clear()\n",
    "        self._seen_tokens.clear()\n",
    "\n",
    "        for e in entries:\n",
    "            orig = e[\"original\"]\n",
    "            token = e[\"token\"]\n",
    "            count = int(e.get(\"count\", 0))\n",
    "            valid = bool(e.get(\"valid\", True))\n",
    "            rec = UserRecord(token=token, count=count, valid=valid)\n",
    "            self._by_orig[orig] = rec\n",
    "            self._token_to_orig[token] = orig\n",
    "            self._seen_tokens.add(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a8c648-9717-40ac-bbeb-ae690287a0c7",
   "metadata": {},
   "source": [
    "#### Json \"export\" ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "507e689b-14ea-42bf-b467-598b7a3b05f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ipv4_re = re.compile(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\")\n",
    "\n",
    "def is_valid_user(u) -> bool:\n",
    "    if pd.isna(u):\n",
    "        return False\n",
    "    s = str(u).strip()\n",
    "    return len(s) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d72af62-7bc8-4974-8e5d-acbc3c16dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_ipv4(ip) -> bool:\n",
    "    if pd.isna(ip):\n",
    "        return False\n",
    "    s = str(ip).strip()\n",
    "    if not _ipv4_re.match(s):\n",
    "        return False\n",
    "    try:\n",
    "        parts = [int(p) for p in s.split(\".\")]\n",
    "    except ValueError:\n",
    "        return False\n",
    "    return all(0 <= p <= 255 for p in parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f978ea1-e247-4cbe-8264-7641b0d59f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_jagged_array(ob_dict: Dict[str, Dict[str, object]]) -> List[List[object]]:\n",
    "    \"\"\"\n",
    "    Anonymized jagged array builder:\n",
    "      input: { original: {\"id\": token, \"count\": int, \"valid\": bool}, ... }\n",
    "      output: [[token, count, valid], ...]   # NO original included\n",
    "    \"\"\"\n",
    "    return [[data[\"id\"], data[\"count\"], data[\"valid\"]]\n",
    "            for _, data in ob_dict.items()] #throwaway with keys to get values in tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dafb811d-8617-48ea-87d1-927883490d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_json(obj, path: str):\n",
    "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f450552d-f357-4a6a-807e-867ffa1e01fb",
   "metadata": {},
   "source": [
    "#### Helpers/Secondary ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14679a93-236b-4535-ac67-71c4c958acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframe(data_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Uses DuckDB to select required columns from all parquet files in data_dir.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(data_dir):\n",
    "        raise FileNotFoundError(f\"DATA_DIR does not exist or is not a directory: {data_dir}\")\n",
    "\n",
    "    pattern = f\"{data_dir}/*.parquet\"\n",
    "    query = (\n",
    "        \"SELECT User, RequestMemory, CumulativeSlotTime, JobsubClientIpAddress, \"\n",
    "        \"MATCH_EXP_JOB_Site, DAG_NodesFailed, NumJobCompletions, NumJobStarts \"\n",
    "        f\"FROM '{pattern}'\"\n",
    "    )\n",
    "    rel_obj = duckdb.sql(query)\n",
    "    return rel_obj.df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc69ebb2-e7a7-4798-a8e2-0408e815bac6",
   "metadata": {},
   "source": [
    "#### Loading Data ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "480a20d2-3da8-4b46-b653-92ecb3d02ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_obfuscations(\n",
    "    df: pd.DataFrame,\n",
    "    user_col: str = USER_COL,\n",
    "    ip_col: str = IP_COL,\n",
    ") -> Tuple[Dict[str, Dict[str, object]], Dict[str, Dict[str, object]], GarbleTokenMapper, GarbleTokenMapper]:\n",
    "    \"\"\"\n",
    "    Build two dicts:\n",
    "      users_dict = { original_user: {\"id\": token, \"count\": n, \"valid\": bool}, ... }\n",
    "      ips_dict   = { original_ip:   {\"id\": token, \"count\": n, \"valid\": bool}, ... }\n",
    "    Returns also the underlying mappers (useful if you want to export the maps).\n",
    "\n",
    "    Tokens are sequential:\n",
    "      - Users: UR1, UR2, ...\n",
    "      - IPs:   IP1, IP2, ...\n",
    "    \"\"\"\n",
    "    user_mapper = GarbleTokenMapper(prefix=\"UR\", start=1)\n",
    "    ip_mapper   = GarbleTokenMapper(prefix=\"IP\", start=1)\n",
    "\n",
    "    # iterate rows to add and count\n",
    "    for _, row in df.iterrows():\n",
    "        u = row.get(user_col)\n",
    "        ip = row.get(ip_col)\n",
    "\n",
    "        user_mapper.add(str(u), valid=is_valid_user(u))\n",
    "        ip_mapper.add(str(ip), valid=is_valid_ipv4(ip))\n",
    "\n",
    "    users_dict = {\n",
    "        orig: {\"id\": rec.token, \"count\": rec.count, \"valid\": rec.valid}\n",
    "        for orig, rec in user_mapper._by_orig.items()\n",
    "    }\n",
    "    ips_dict = {\n",
    "        orig: {\"id\": rec.token, \"count\": rec.count, \"valid\": rec.valid}\n",
    "        for orig, rec in ip_mapper._by_orig.items()\n",
    "    }\n",
    "    return users_dict, ips_dict, user_mapper, ip_mapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a422fc2-fdab-403d-8efb-f4fc92b5ee16",
   "metadata": {},
   "source": [
    "#### Transform ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6856717-5d06-44e1-a82b-cd8cde84ed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_output_json(\n",
    "    df: pd.DataFrame,\n",
    "    users_dict: Dict[str, Dict[str, object]],\n",
    "    ips_dict: Dict[str, Dict[str, object]],\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build the final JSON payload that includes jagged arrays and minimal metadata.\n",
    "    \"\"\"\n",
    "    users_jagged = to_jagged_array(users_dict)\n",
    "    ips_jagged   = to_jagged_array(ips_dict)\n",
    "\n",
    "    payload = {\n",
    "        \"users\": users_jagged,   # [[original, token, count, valid], ...]\n",
    "        \"ips\":   ips_jagged,     # [[original, token, count, valid], ...]\n",
    "        \"meta\": {\n",
    "            \"total_rows\": int(len(df)),\n",
    "            \"distinct_users\": int(len(users_dict)),\n",
    "            \"distinct_ips\": int(len(ips_dict)),\n",
    "        },\n",
    "    }\n",
    "    return json.dumps(payload, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f536cb23-30ed-4dfc-b8a5-131c24a5597f",
   "metadata": {},
   "source": [
    "#### Generic User json (Below) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08a5b8be-bf92-40cb-9263-f314ea019d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_summary_payload(\n",
    "    df: pd.DataFrame,\n",
    "    users_dict: Dict[str, Dict[str, object]],\n",
    "    ips_dict: Dict[str, Dict[str, object]],\n",
    "    user_col: str = USER_COL,\n",
    "    ip_col: str = IP_COL,\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Build the summary payload and include per-user/IP correlation records:\n",
    "      - users: [[token, count, valid], ...]               (no originals)\n",
    "      - ips:   [[token, count, valid], ...]               (no originals)\n",
    "      - user_ip_correlations: [\n",
    "            [original_user, user_token, original_ip, ip_token, frequency, user_valid],\n",
    "            ...\n",
    "        ]\n",
    "    \"\"\"\n",
    "\n",
    "    # Anonymized jagged arrays (no originals)\n",
    "    users_jagged_anon = [[d[\"id\"], d[\"count\"], d[\"valid\"]] for d in users_dict.values()]\n",
    "    ips_jagged_anon   = [[d[\"id\"], d[\"count\"], d[\"valid\"]] for d in ips_dict.values()]\n",
    "\n",
    "    # Compute, for each user, the most frequent (mode) IP they used\n",
    "    def _pick_mode_ip(series: pd.Series) -> Optional[str]:\n",
    "        ser = series.dropna().astype(str)\n",
    "        if ser.empty:\n",
    "            return None\n",
    "        return ser.value_counts().idxmax()\n",
    "\n",
    "    tmp = df[[user_col, ip_col]].copy()\n",
    "    tmp[user_col] = tmp[user_col].astype(str)\n",
    "    top_ip_for_user = tmp.groupby(user_col)[ip_col].apply(_pick_mode_ip)\n",
    "\n",
    "    # Build correlated records\n",
    "    user_ip_correlations = []\n",
    "    for orig_user, udata in users_dict.items():\n",
    "        key_user = str(orig_user)\n",
    "        ip_orig = top_ip_for_user.get(key_user, None)\n",
    "        ip_token = ips_dict.get(ip_orig, {}).get(\"id\") if ip_orig is not None else None\n",
    "        user_ip_correlations.append([\n",
    "            key_user,                 # original user\n",
    "            udata[\"id\"],              # garbled user\n",
    "            ip_orig,                  # user's (mode) IP original\n",
    "            ip_token,                 # garbled IP\n",
    "            int(udata[\"count\"]),      # frequency (user count)\n",
    "            bool(udata[\"valid\"]),     # user validity\n",
    "        ])\n",
    "\n",
    "    return {\n",
    "        \"users\": users_jagged_anon,\n",
    "        \"ips\": ips_jagged_anon,\n",
    "        \"user_ip_correlations\": user_ip_correlations,\n",
    "        \"meta\": {\n",
    "            \"total_rows\": int(len(df)),\n",
    "            \"distinct_users\": int(len(users_dict)),\n",
    "            \"distinct_ips\": int(len(ips_dict)),\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ae540-7274-42da-84c3-5c8da268f421",
   "metadata": {},
   "source": [
    "#### Failed User json (Below) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "314574ec-e2dc-42f9-821d-7124a904edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def failed_users_payload(\n",
    "    df: pd.DataFrame,\n",
    "    user_mapper: GarbleTokenMapper,\n",
    "    user_col: str = USER_COL,\n",
    "    starts_col: str = NUM_STARTS_COL,\n",
    "    completions_col: str = NUM_COMPLETIONS_COL,\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Create a payload listing JUST users with failed jobs, WITHOUT originals.\n",
    "    Each record: { \"token\": <str>, \"failure_count\": <int>, \"valid\": <bool> }\n",
    "    \"\"\"\n",
    "    if not {user_col, starts_col, completions_col} <= set(df.columns):\n",
    "        return {\n",
    "            \"failed_users\": [],\n",
    "            \"meta\": {\n",
    "                \"distinct_failed_users\": 0,\n",
    "                \"total_failure_rows\": 0,\n",
    "                \"note\": \"Required columns missing; cannot compute failed users.\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    mask_fail = (df[completions_col].astype(\"int\") == 0) & (df[starts_col] > 0)\n",
    "    failed_df = df.loc[mask_fail, [user_col]]\n",
    "\n",
    "    # count failure rows per user\n",
    "    fail_counts = failed_df.groupby(user_col)[user_col].count().rename(\"failure_count\")\n",
    "\n",
    "    records = []\n",
    "    total_failure_rows = int(fail_counts.sum()) if not fail_counts.empty else 0\n",
    "\n",
    "    for orig_user, fcount in fail_counts.items():\n",
    "        token = user_mapper.add(str(orig_user), valid=is_valid_user(orig_user))\n",
    "        records.append({\n",
    "            \"token\": token,\n",
    "            \"failure_count\": int(fcount),\n",
    "            \"valid\": is_valid_user(orig_user),\n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"failed_users\": records,\n",
    "        \"meta\": {\n",
    "            \"distinct_failed_users\": int(len(records)),\n",
    "            \"total_failure_rows\": total_failure_rows,\n",
    "        },\n",
    "    }\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4339a7a2-11fa-4f14-b42e-f7afc79e5eb3",
   "metadata": {},
   "source": [
    "#### Full output ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d28d6d55-9ca7-40e9-bdaf-3bdcd06cac53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Small samples ===\n",
      "users_dict sample: {\n",
      "  \"uboonepro@fnal.gov\": {\n",
      "    \"id\": \"UR1\",\n",
      "    \"count\": 99239,\n",
      "    \"valid\": true\n",
      "  },\n",
      "  \"icaruspro@fnal.gov\": {\n",
      "    \"id\": \"UR2\",\n",
      "    \"count\": 47080,\n",
      "    \"valid\": true\n",
      "  },\n",
      "  \"gputnam@fnal.gov\": {\n",
      "    \"id\": \"UR3\",\n",
      "    \"count\": 12693,\n",
      "    \"valid\": true\n",
      "  }\n",
      "}\n",
      "ips_dict sample  : {\n",
      "  \"131.225.240.146\": {\n",
      "    \"id\": \"IP1\",\n",
      "    \"count\": 86225,\n",
      "    \"valid\": true\n",
      "  },\n",
      "  \"131.225.240.90\": {\n",
      "    \"id\": \"IP2\",\n",
      "    \"count\": 47080,\n",
      "    \"valid\": true\n",
      "  },\n",
      "  \"131.225.240.140\": {\n",
      "    \"id\": \"IP3\",\n",
      "    \"count\": 12693,\n",
      "    \"valid\": true\n",
      "  }\n",
      "}\n",
      "\n",
      "summary.json preview:\n",
      " {\n",
      "  \"users\": [\n",
      "    [\n",
      "      \"UR1\",\n",
      "      99239,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR2\",\n",
      "      47080,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR3\",\n",
      "      12693,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR4\",\n",
      "      3652,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR5\",\n",
      "      15298,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR6\",\n",
      "      621,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR7\",\n",
      "      28340,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR8\",\n",
      "      1508,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR9\",\n",
      "      329,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR10\",\n",
      "      6867,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR11\",\n",
      "      417,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR12\",\n",
      "      105,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR13\",\n",
      "      52,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR14\",\n",
      "      42,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR15\",\n",
      "      108,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR16\",\n",
      "      17,\n",
      "      true\n",
      "    ],\n",
      " ...\n",
      "\n",
      "Job failure fraction %: 9.581%, job failure abs number: 24400\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    df = load_dataframe(DATA_DIR)\n",
    "\n",
    "    # Build obfuscations\n",
    "    users_dict, ips_dict, user_mapper, ip_mapper = build_obfuscations(\n",
    "        df, user_col=USER_COL, ip_col=IP_COL\n",
    "    )\n",
    "\n",
    "    # --- Create payloads / JSON strings ---\n",
    "    summary_obj = make_summary_payload(df, users_dict, ips_dict)\n",
    "    summary_json = json.dumps(summary_obj, indent=2)\n",
    "\n",
    "    # Explicit JSON for the jagged arrays (as requested)\n",
    "    users_jagged = to_jagged_array(users_dict)\n",
    "    ips_jagged   = to_jagged_array(ips_dict)\n",
    "    users_jagged_obj = {\n",
    "        \"users\": users_jagged,\n",
    "        \"meta\": {\n",
    "            \"distinct_users\": int(len(users_dict)),\n",
    "            \"total_rows\": int(len(df)),\n",
    "        },\n",
    "    }\n",
    "    ips_jagged_obj = {\n",
    "        \"ips\": ips_jagged,\n",
    "        \"meta\": {\n",
    "            \"distinct_ips\": int(len(ips_dict)),\n",
    "            \"total_rows\": int(len(df)),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Failed users payload (JUST the users with failed jobs)\n",
    "    failed_obj = failed_users_payload(\n",
    "        df,\n",
    "        user_mapper=user_mapper,\n",
    "        user_col=USER_COL,\n",
    "        starts_col=NUM_STARTS_COL,\n",
    "        completions_col=NUM_COMPLETIONS_COL,\n",
    "    )\n",
    "\n",
    "    # --- Write files ---\n",
    "    summary_path = os.path.join(OUTPUT_DIR, \"summary.json\")\n",
    "    users_jagged_path = os.path.join(OUTPUT_DIR, \"users_jagged.json\")\n",
    "    ips_jagged_path = os.path.join(OUTPUT_DIR, \"ips_jagged.json\")\n",
    "    failed_users_path = os.path.join(OUTPUT_DIR, \"failed_users.json\")\n",
    "\n",
    "    \n",
    "    dump_json(summary_obj, summary_path)\n",
    "    dump_json(users_jagged_obj, users_jagged_path)\n",
    "    dump_json(ips_jagged_obj, ips_jagged_path)\n",
    "    dump_json(failed_obj, failed_users_path)\n",
    "    \n",
    "    # --- Console peeks ---\n",
    "    #print(\"\\n=== Wrote JSON files ===\")\n",
    "    #print(\"summary         :\", summary_path)\n",
    "    #print(\"users_jagged    :\", users_jagged_path)\n",
    "    #print(\"ips_jagged      :\", ips_jagged_path)\n",
    "    #print(\"failed_users    :\", failed_users_path)\n",
    "\n",
    "    print(\"\\n=== Small samples ===\")\n",
    "    print(\"users_dict sample:\", json.dumps(dict(list(users_dict.items())[:3]), indent=2))\n",
    "    print(\"ips_dict sample  :\", json.dumps(dict(list(ips_dict.items())[:3]), indent=2))\n",
    "    print(\"\\nsummary.json preview:\\n\", summary_json[:800], \"...\\n\")\n",
    "\n",
    "    # --- Failure stats (for context) ---\n",
    "    total_starts = int(df[NUM_STARTS_COL].sum()) if NUM_STARTS_COL in df else 0\n",
    "    total_completions = int(df[NUM_COMPLETIONS_COL].astype(\"int\").sum()) if NUM_COMPLETIONS_COL in df else 0\n",
    "    n_job_failures = total_starts - total_completions\n",
    "    job_failure_frac = (n_job_failures / total_starts) if total_starts else 0.0\n",
    "    print(f\"Job failure fraction %: {job_failure_frac:.3%}, job failure abs number: {n_job_failures}\") #Job failures = (Jobs Started - Jobs finished)/ Jobs Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d3efae-e9a1-4d22-8cb4-06375e6803bf",
   "metadata": {},
   "source": [
    "#### Main ####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
