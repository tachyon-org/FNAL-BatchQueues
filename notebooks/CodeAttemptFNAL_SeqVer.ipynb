{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7b80bbb-dd2a-4ed6-8f12-7627ad56a48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nathan/git/fnal-proj/notebooks/CodeAttemptFNAL_SeqVer.py\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# set your filename if needed\n",
    "file_path = Path(\"CodeAttemptFNAL_SeqVer.py\").resolve()\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f4f76f-e634-488f-88fa-09bbf36e7834",
   "metadata": {},
   "source": [
    "##### Note: Code split between two parts for me. First part is gathering data to put in a way to use, second part is transforming it. Also, headers are below code. ##### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1ca27c-51a0-419c-9235-22031ee4dfcb",
   "metadata": {},
   "source": [
    "## Methods ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f2d68-5037-416b-ad30-b653ebedbbe4",
   "metadata": {},
   "source": [
    "\n",
    "(Garble Methods)\n",
    "\n",
    "\n",
    "\n",
    "add — returns a stable token for an original and increments its count.\n",
    "\n",
    "original_from_token — reverse-maps a token back to the original string.\n",
    "\n",
    "record_from_token — fetches the UserRecord associated with a token.\n",
    "\n",
    "export_to_json — saves the mapper’s current state to disk.\n",
    "\n",
    "load_from_json — restores the mapper’s state from a saved JSON file.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(Helper Methods)\n",
    "is_valid_user — checks that a user value is non-null and non-empty.\n",
    "\n",
    "is_valid_ipv4 — validates IPv4 dotted-quad format and range.\n",
    "\n",
    "to_jagged_array — builds [[original, token, count, valid], ...] for non-anonymous use.\n",
    "\n",
    "dump_json — writes a Python object to a pretty-printed JSON file.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(Data Methods)\n",
    "load_dataframe — selects required columns from all Parquet files in DATA_DIR.\n",
    "\n",
    "build_obfuscations — iterates rows to create user/IP token maps with counts/validity.\n",
    "\n",
    "make_summary_payload — returns anonymized users/IPs jagged arrays plus meta.\n",
    "\n",
    "failed_users_payload — returns anonymized records for users with failed jobs plus meta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "825b86e0-dcf0-4f5c-8084-62db4becd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import string\n",
    "import secrets\n",
    "from dataclasses import dataclass, asdict #json helper\n",
    "from typing import Dict, Optional, List, Tuple #typing helper\n",
    "#transform data\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "#data I/O\n",
    "\n",
    "\n",
    "from textwrap import fill, indent\n",
    "#For readable texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cd0ac9a-fdec-490d-b524-24fd8ebe5740",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "OUTPUT_DIR = \"./Output\"\n",
    "HUMAN_WRAP = 100 # wrap width for text output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4385f10c-4a5f-41b2-ba35-5fd7bac11178",
   "metadata": {},
   "source": [
    "### Import stuff ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06a924c1-dddc-4b77-8ca3-f6af01587dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_COL   = \"User\"\n",
    "IP_COL     = \"JobsubClientIpAddress\"\n",
    "FAILED_COL = \"DAG_NodesFailed\"  # “boolean-ish”\n",
    "NUM_STARTS_COL     = \"NumJobStarts\"\n",
    "NUM_COMPLETIONS_COL= \"NumJobCompletions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bb0771-4508-4180-b4fd-0c5ec24d0be2",
   "metadata": {},
   "source": [
    "### Config / Column Names ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f1d274a-6a98-4347-90da-656668de7b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIGITS = string.digits\n",
    "LOWER = string.ascii_lowercase\n",
    "UPPER = string.ascii_uppercase\n",
    "DEFAULT_PUNCT = \"!#$%&()*+,-.:;<=>?@[]^_{|}~\"\n",
    "CHAR_TYPE_CHOICES = [\"digit\", \"lower\", \"upper\", \"punct\"]\n",
    "#obfuscation types\n",
    "\n",
    "@dataclass #compact record\n",
    "class UserRecord:\n",
    "    token: str\n",
    "    count: int\n",
    "    valid: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8e9059c-4f51-4314-bbf4-b261fdc6aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GarbleTokenMapper:\n",
    "   \n",
    "    def __init__(\n",
    "        self,\n",
    "        prefix: str = \"\",\n",
    "        start: int = 1,\n",
    "        # legacy args kept; ignored\n",
    "        token_len: int = 8,\n",
    "\n",
    "        allow_punctuation: bool = False,\n",
    "        punct_chars: Optional[str] = None,\n",
    "    ):\n",
    "        self.prefix = str(prefix or \"\")\n",
    "        self.start = int(start)\n",
    "        # original into UserRecord\n",
    "        self._by_orig: Dict[str, UserRecord] = {}\n",
    "        # token into original\n",
    "        self._token_to_orig: Dict[str, str] = {}\n",
    "        # issued tokens for parity\n",
    "        self._seen_tokens = set()\n",
    "        # counter points +1\n",
    "        self._counter = self.start - 1\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_trailing_int(s: str) -> Optional[int]:\n",
    "        m = re.search(r\"(\\d+)$\", str(s))\n",
    "        return int(m.group(1)) if m else None\n",
    "\n",
    "    def _next_token(self) -> str:\n",
    "        self._counter += 1\n",
    "        return f\"{self.prefix}{self._counter}\"\n",
    "\n",
    "    def add(self, original: str, valid: bool = True) -> str:\n",
    "        key = str(original)\n",
    "        if key in self._by_orig:\n",
    "            rec = self._by_orig[key]\n",
    "            rec.count += 1\n",
    "            return rec.token\n",
    "\n",
    "        token = self._next_token()\n",
    "        self._seen_tokens.add(token)\n",
    "        rec = UserRecord(token=token, count=1, valid=bool(valid))\n",
    "        self._by_orig[key] = rec\n",
    "        self._token_to_orig[token] = key\n",
    "        return token\n",
    "\n",
    "    def original_from_token(self, token: str) -> Optional[str]:\n",
    "        return self._token_to_orig.get(str(token))\n",
    "\n",
    "    def record_from_token(self, token: str) -> Optional[UserRecord]:\n",
    "        orig = self._token_to_orig.get(str(token))\n",
    "        return self._by_orig.get(orig) if orig is not None else None\n",
    "\n",
    "    def export_to_json(self, filepath: str) -> None:\n",
    "        entries = []\n",
    "        for orig, rec in self._by_orig.items():\n",
    "            e = asdict(rec)\n",
    "            e[\"original\"] = orig\n",
    "            entries.append(e)\n",
    "        state = {\n",
    "            \"entries\": entries,\n",
    "            \"config\": {\"prefix\": self.prefix, \"start\": self.start, \"counter\": self._counter},\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(filepath) or \".\", exist_ok=True)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(state, f, indent=2)\n",
    "\n",
    "    def load_from_json(self, filepath: str) -> None:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        entries = data.get(\"entries\", [])\n",
    "        cfg = data.get(\"config\", {})\n",
    "\n",
    "        # reset\n",
    "        self._by_orig.clear()\n",
    "        self._token_to_orig.clear()\n",
    "        self._seen_tokens.clear()\n",
    "\n",
    "        # keep existing prefix/start unless given(in file)\n",
    "        self.prefix = str(cfg.get(\"prefix\", self.prefix))\n",
    "        self.start = int(cfg.get(\"start\", self.start))\n",
    "\n",
    "        max_num = self.start - 1\n",
    "        for e in entries:\n",
    "            orig = str(e[\"original\"])\n",
    "            token = str(e[\"token\"])\n",
    "            count = int(e.get(\"count\", 0))\n",
    "            valid = bool(e.get(\"valid\", True))\n",
    "            rec = UserRecord(token=token, count=count, valid=valid)\n",
    "            self._by_orig[orig] = rec\n",
    "            self._token_to_orig[token] = orig\n",
    "            self._seen_tokens.add(token)\n",
    "            n = self._extract_trailing_int(token)\n",
    "            if n is not None:\n",
    "                max_num = max(max_num, n)\n",
    "\n",
    "        # count after max number\n",
    "        self._counter = int(cfg.get(\"counter\", max_num))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5a0b5-d1aa-4158-b1ce-887313a16711",
   "metadata": {},
   "source": [
    "### Token Mapper (Part 2) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e2670f8-efef-4c42-9bc3-ba724f48830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_json(self, filepath: str):\n",
    "        entries = []\n",
    "        for orig, rec in self._by_orig.items():\n",
    "            e = asdict(rec)\n",
    "            e[\"original\"] = orig\n",
    "            entries.append(e)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"entries\": entries}, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a8c648-9717-40ac-bbeb-ae690287a0c7",
   "metadata": {},
   "source": [
    "#### Json \"export\" ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "507e689b-14ea-42bf-b467-598b7a3b05f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ipv4_re = re.compile(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\")\n",
    "\n",
    "def is_valid_user(u) -> bool:\n",
    "    if pd.isna(u):\n",
    "        return False\n",
    "    s = str(u).strip()\n",
    "    return len(s) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "78be3532-2ad3-429f-a101-0d2b01370169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_ipv4(ip) -> bool:\n",
    "    if pd.isna(ip):\n",
    "        return False\n",
    "\n",
    "    s = str(ip).strip()\n",
    "    if not _ipv4_re.match(s):\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        parts = [int(p) for p in s.split(\".\")]\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "    return all(0 <= p <= 255 for p in parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f978ea1-e247-4cbe-8264-7641b0d59f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_jagged_array(ob_dict: Dict[str, Dict[str, object]]) -> List[List[object]]:\n",
    "\n",
    "    return [[data[\"id\"], data[\"count\"], data[\"valid\"]]\n",
    "            for _, data in ob_dict.items()] #throwaway with keys to get values in tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dafb811d-8617-48ea-87d1-927883490d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_json(obj, path: str):\n",
    "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f450552d-f357-4a6a-807e-867ffa1e01fb",
   "metadata": {},
   "source": [
    "#### Helpers/Secondary ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14679a93-236b-4535-ac67-71c4c958acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframe(data_dir: str) -> pd.DataFrame:\n",
    " \n",
    "    if not os.path.isdir(data_dir):\n",
    "        raise FileNotFoundError(f\"DATA_DIR does not exist or is not a directory: {data_dir}\")\n",
    "\n",
    "    pattern = f\"{data_dir}/*.parquet\"\n",
    "\n",
    "    desired_cols = [\n",
    "        \"User\",\n",
    "        \"RequestMemory\",\n",
    "        \"CumulativeSlotTime\",\n",
    "        \"JobsubClientIpAddress\",\n",
    "        \"MATCH_EXP_JOB_Site\",\n",
    "        \"DAG_NodesFailed\",\n",
    "        \"NumJobCompletions\",\n",
    "        \"NumJobStarts\",\n",
    "        \"Cmd\",\n",
    "        \"Environment\",\n",
    "    ]\n",
    "\n",
    "    # Learn col. across files\n",
    "    schema_df = duckdb.sql(f\"SELECT * FROM read_parquet('{pattern}') LIMIT 0\").df()\n",
    "    available = set(schema_df.columns)\n",
    "\n",
    "    present = [c for c in desired_cols if c in available]\n",
    "    missing = [c for c in desired_cols if c not in available]\n",
    "    if missing:\n",
    "        print(f\"[load_dataframe] Warning: missing columns not found in any file: {missing}\")\n",
    "\n",
    "    if not present:\n",
    "        raise RuntimeError(\"None of the desired columns are present in the parquet files.\")\n",
    "\n",
    "    q = \", \".join([f'\"{c}\"' for c in present])\n",
    "    query = f\"SELECT {q} FROM read_parquet('{pattern}')\"\n",
    "    return duckdb.sql(query).df()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc69ebb2-e7a7-4798-a8e2-0408e815bac6",
   "metadata": {},
   "source": [
    "#### Loading Data ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "480a20d2-3da8-4b46-b653-92ecb3d02ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_obfuscations(\n",
    "    df: pd.DataFrame,\n",
    "    user_col: str = USER_COL,\n",
    "    ip_col: str = IP_COL,\n",
    ") -> Tuple[Dict[str, Dict[str, object]], Dict[str, Dict[str, object]], GarbleTokenMapper, GarbleTokenMapper]:\n",
    "   \n",
    "    user_mapper = GarbleTokenMapper(prefix=\"UR\", start=1)\n",
    "    ip_mapper   = GarbleTokenMapper(prefix=\"IP\", start=1)\n",
    "\n",
    "    # iterate rows to add and count\n",
    "    for _, row in df.iterrows():\n",
    "        u = row.get(user_col)\n",
    "        ip = row.get(ip_col)\n",
    "\n",
    "        user_mapper.add(str(u), valid=is_valid_user(u))\n",
    "        ip_mapper.add(str(ip), valid=is_valid_ipv4(ip))\n",
    "\n",
    "    users_dict = {\n",
    "        orig: {\"id\": rec.token, \"count\": rec.count, \"valid\": rec.valid}\n",
    "        for orig, rec in user_mapper._by_orig.items()\n",
    "    }\n",
    "    ips_dict = {\n",
    "        orig: {\"id\": rec.token, \"count\": rec.count, \"valid\": rec.valid}\n",
    "        for orig, rec in ip_mapper._by_orig.items()\n",
    "    }\n",
    "    return users_dict, ips_dict, user_mapper, ip_mapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a422fc2-fdab-403d-8efb-f4fc92b5ee16",
   "metadata": {},
   "source": [
    "#### Transform ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6856717-5d06-44e1-a82b-cd8cde84ed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_output_json(\n",
    "    df: pd.DataFrame,\n",
    "    users_dict: Dict[str, Dict[str, object]],\n",
    "    ips_dict: Dict[str, Dict[str, object]],\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build the final JSON payload that includes jagged arrays and minimal metadata.\n",
    "    \"\"\"\n",
    "    users_jagged = to_jagged_array(users_dict)\n",
    "    ips_jagged   = to_jagged_array(ips_dict)\n",
    "\n",
    "    payload = {\n",
    "        \"users\": users_jagged,   \n",
    "        \"ips\":   ips_jagged,  \n",
    "        \"meta\": {\n",
    "            \"total_rows\": int(len(df)),\n",
    "            \"distinct_users\": int(len(users_dict)),\n",
    "            \"distinct_ips\": int(len(ips_dict)),\n",
    "        },\n",
    "    }\n",
    "    return json.dumps(payload, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f536cb23-30ed-4dfc-b8a5-131c24a5597f",
   "metadata": {},
   "source": [
    "#### Generic User json (Below) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "08a5b8be-bf92-40cb-9263-f314ea019d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_summary_payload(\n",
    "    df: pd.DataFrame,\n",
    "    users_dict: Dict[str, Dict[str, object]],\n",
    "    ips_dict: Dict[str, Dict[str, object]],\n",
    "    user_col: str = USER_COL,\n",
    "    ip_col: str = IP_COL,\n",
    ") -> Dict[str, object]:\n",
    "   \n",
    "\n",
    "    # Anonymized jagged arrays (no originals)\n",
    "    users_jagged_anon = [[d[\"id\"], d[\"count\"], d[\"valid\"]] for d in users_dict.values()]\n",
    "    ips_jagged_anon   = [[d[\"id\"], d[\"count\"], d[\"valid\"]] for d in ips_dict.values()]\n",
    "\n",
    "    # Compute, for each user, the most frequent (mode) IP they used\n",
    "    def _pick_mode_ip(series: pd.Series) -> Optional[str]:\n",
    "        ser = series.dropna().astype(str)\n",
    "        if ser.empty:\n",
    "            return None\n",
    "        return ser.value_counts().idxmax()\n",
    "\n",
    "    tmp = df[[user_col, ip_col]].copy()\n",
    "    tmp[user_col] = tmp[user_col].astype(str)\n",
    "    top_ip_for_user = tmp.groupby(user_col)[ip_col].apply(_pick_mode_ip)\n",
    "\n",
    "    # Build correlated records\n",
    "    user_ip_correlations = []\n",
    "    for orig_user, udata in users_dict.items():\n",
    "        key_user = str(orig_user)\n",
    "        ip_orig = top_ip_for_user.get(key_user, None)\n",
    "        ip_token = ips_dict.get(ip_orig, {}).get(\"id\") if ip_orig is not None else None\n",
    "        user_ip_correlations.append([\n",
    "            key_user,                 # original user\n",
    "            udata[\"id\"],              # garbled user\n",
    "            ip_orig,                  # user's (mode) IP original\n",
    "            ip_token,                 # garbled IP\n",
    "            int(udata[\"count\"]),      # frequency (user count)\n",
    "            bool(udata[\"valid\"]),     # user validity\n",
    "        ])\n",
    "\n",
    "    return {\n",
    "        \"users\": users_jagged_anon,\n",
    "        \"ips\": ips_jagged_anon,\n",
    "        \"user_ip_correlations\": user_ip_correlations,\n",
    "        \"meta\": {\n",
    "            \"total_rows\": int(len(df)),\n",
    "            \"distinct_users\": int(len(users_dict)),\n",
    "            \"distinct_ips\": int(len(ips_dict)),\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06a4b728-b376-4b69-aeb5-21948dd350c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _s(x):\n",
    "    #in case of NaN/none\n",
    "    return \"\" if pd.isna(x) else str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "320eb95c-74f7-4eab-b87e-1ab6f13ff259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_env(env_raw):\n",
    "\n",
    "    def _sort_key(pair):\n",
    "        #Case sens.\n",
    "        return pair[0].lower()\n",
    "\n",
    "    s = _s(env_raw).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "\n",
    "    # Json\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, dict):\n",
    "            pairs = []\n",
    "            for k, v in obj.items():\n",
    "                val = \"\" if v is None else str(v)\n",
    "                pairs.append((str(k), val))\n",
    "            return sorted(pairs, key=_sort_key)\n",
    "\n",
    "        # Json arrays of KEY=VAL or dicts.\n",
    "        if isinstance(obj, list):\n",
    "            pairs = []\n",
    "            for item in obj:\n",
    "                if isinstance(item, dict):\n",
    "                    for k, v in item.items():\n",
    "                        val = \"\" if v is None else str(v)\n",
    "                        pairs.append((str(k), val))\n",
    "                elif isinstance(item, str) and \"=\" in item:\n",
    "                    k, v = item.split(\"=\", 1)\n",
    "                    pairs.append((k.strip(), v.strip()))\n",
    "                else:\n",
    "                    pairs.append((\"ITEM\", str(item)))\n",
    "            return sorted(pairs, key=_sort_key)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Dict.\n",
    "    if (s.startswith(\"{\") and s.endswith(\"}\")) or (s.startswith(\"dict(\") and s.endswith(\")\")):\n",
    "        try:\n",
    "            s_jsonish = s.replace(\"'\", \"\\\"\")\n",
    "            obj = json.loads(s_jsonish)\n",
    "            if isinstance(obj, dict):\n",
    "                pairs = []\n",
    "                for k, v in obj.items():\n",
    "                    val = \"\" if v is None else str(v)\n",
    "                    pairs.append((str(k), val))\n",
    "                return sorted(pairs, key=_sort_key)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # KEY=VAL pars.\n",
    "    candidates = []\n",
    "    for delim in [\";\", \",\", \"\\n\"]:\n",
    "        if delim in s:\n",
    "            candidates = [p for p in s.split(delim)]\n",
    "            break\n",
    "    if not candidates:\n",
    "        # space-separated tokens; keep tokens that look like KEY=VAL\n",
    "        candidates = s.split()\n",
    "\n",
    "    pairs = []\n",
    "    for token in candidates:\n",
    "        token = token.strip()\n",
    "        if not token:\n",
    "            continue\n",
    "        if \"=\" in token:\n",
    "            k, v = token.split(\"=\", 1)\n",
    "            pairs.append((k.strip(), v.strip()))\n",
    "    if pairs:\n",
    "        return sorted(pairs, key=_sort_key)\n",
    "\n",
    "    # Fallback\n",
    "    return [(\"ENV\", s)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70f1731d-7518-47d2-b997-f5ba764af31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _wrap_block(text, width):\n",
    "    return fill(_s(text), width=width, replace_whitespace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e35aa5e-596d-4c06-ac71-058a1d3d6d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_env_block(env_pairs, width, indent_spaces=2):\n",
    "    if not env_pairs:\n",
    "        return \"  (none)\"\n",
    "    lines = []\n",
    "    for k, v in env_pairs:\n",
    "        # \"KEY=VALUE\" with wrapping of the value\n",
    "        if v:\n",
    "            wrapped_v = fill(v, width=width - (len(k) + 1 + indent_spaces),\n",
    "                             subsequent_indent=\" \" * (len(k) + 1))\n",
    "            lines.append(f\"{k}={wrapped_v}\")\n",
    "        else:\n",
    "            lines.append(f\"{k}=\")\n",
    "    return indent(\"\\n\".join(lines), \" \" * indent_spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3143de-7264-48ec-92f4-980b82bf0d63",
   "metadata": {},
   "source": [
    "### Helpers ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8264413-ad58-45d8-aa00-f50de652ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWN_EXPERIMENTS = {\"uboone\", \"icarus\", \"pip2\", \"nova\", \"dune\"}\n",
    "\n",
    "def _extract_user_handle(user_val: str) -> str:\n",
    "    \"\"\"Return local-part before '@' if it's an email-like string; else a stripped token.\"\"\"\n",
    "    s = _s(user_val)\n",
    "    if \"@\" in s:\n",
    "        return s.split(\"@\", 1)[0].strip()\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac0da288-1c4b-42b1-9369-4cb8da797680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sensitive_mappers_for_df(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    user_col: str = USER_COL,\n",
    "    env_col: str = \"Environment\",\n",
    "    user_prefix: str = \"UR_\",\n",
    "    exp_prefix: str = \"EX_\",\n",
    ") -> tuple[GarbleTokenMapper, GarbleTokenMapper, dict[str, str], dict[str, str]]:\n",
    "    user_mapper = GarbleTokenMapper(prefix=user_prefix, start=1)\n",
    "    exp_mapper  = GarbleTokenMapper(prefix=exp_prefix, start=1)\n",
    "\n",
    "    handles = set()\n",
    "    experiments = set()\n",
    "\n",
    "    if user_col in df.columns:\n",
    "        for u in df[user_col].dropna().astype(str):\n",
    "            h = _extract_user_handle(u)\n",
    "            if h:\n",
    "                handles.add(h)\n",
    "\n",
    "    # Gather experiments from Environment and known set\n",
    "    if env_col in df.columns:\n",
    "        for raw in df[env_col].dropna().astype(str):\n",
    "            # Try to parse\n",
    "            pairs = _parse_env(raw)\n",
    "            # look for explicit exp.\n",
    "            for k, v in pairs:\n",
    "                if k.upper() == \"EXPERIMENT\" and v:\n",
    "                    experiments.add(v.strip())\n",
    "            # capture known experiment tokens present anywhere in text\n",
    "            low = raw.lower()\n",
    "            for ex in KNOWN_EXPERIMENTS:\n",
    "                if ex in low:\n",
    "                    experiments.add(ex)\n",
    "\n",
    "    # Add known experiments even if not seen\n",
    "    experiments |= KNOWN_EXPERIMENTS\n",
    "\n",
    "    # Build sequential tokens\n",
    "    user_handle_map = {}\n",
    "    for h in sorted(handles, key=lambda s: (len(s), s)):  # deterministic\n",
    "        tok = user_mapper.add(h, valid=True)\n",
    "        user_handle_map[h] = tok\n",
    "\n",
    "    experiment_map = {}\n",
    "    for ex in sorted(experiments, key=lambda s: (len(s), s)):\n",
    "        tok = exp_mapper.add(ex, valid=True)\n",
    "        experiment_map[ex] = tok\n",
    "\n",
    "    return user_mapper, exp_mapper, user_handle_map, experiment_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ffe3a2c7-92c9-4569-af8b-aa567dc20c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_greedy_sub_regex(literals: list[str]) -> re.Pattern:\n",
    "    if not literals:\n",
    "        # Match nothing\n",
    "        return re.compile(r\"(?!x)x\")\n",
    "    # longest first so longer alternatives get tried before their substrings\n",
    "    escaped = [re.escape(s) for s in sorted(literals, key=len, reverse=True)]\n",
    "    return re.compile(\"(\" + \"|\".join(escaped) + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c6686622-e569-49b3-affe-4bfa30939729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_replace(text: str, mapping: dict[str, str], pattern: re.Pattern) -> str:\n",
    "    \n",
    "    if not text:\n",
    "        return text\n",
    "    def _sub(m):\n",
    "        orig = m.group(0)\n",
    "        return mapping.get(orig, orig)\n",
    "    return pattern.sub(_sub, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b9a1240-4d53-4543-b52e-dfe7b0844806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def garble_user_email(email: str, user_handle_map: dict[str, str], pat: re.Pattern) -> str:\n",
    "    #Replace local-part before '@' using greedy mapping\n",
    "    s = _s(email)\n",
    "    if \"@\" not in s:\n",
    " #treat whole string as a handle container\n",
    "        return greedy_replace(s, user_handle_map, pat)\n",
    "    local, domain = s.split(\"@\", 1)\n",
    "    new_local = greedy_replace(local, user_handle_map, pat)\n",
    "    return f\"{new_local}@{domain}\"\n",
    "\n",
    "def garble_row_fields(\n",
    "    row: pd.Series,\n",
    "    *,\n",
    "    user_col: str = USER_COL,\n",
    "    cmd_col: str = \"Cmd\",\n",
    "    env_col: str = \"Environment\",\n",
    "    user_handle_map: dict[str, str],\n",
    "    experiment_map: dict[str, str],\n",
    "    pat_user: re.Pattern,\n",
    "    pat_user_anywhere: re.Pattern,   # same as pat_user (optional separate), used across other fields\n",
    "    pat_exp: re.Pattern,\n",
    ") -> dict:\n",
    "    out = row.to_dict()\n",
    "\n",
    "    # 1) User email (local-part)\n",
    "    if user_col in row.index:\n",
    "        out[user_col] = garble_user_email(_s(row[user_col]), user_handle_map, pat_user)\n",
    "\n",
    "    # 2) Cmd (global)\n",
    "    if cmd_col in row.index and pd.notna(row[cmd_col]):\n",
    "        s = _s(row[cmd_col])\n",
    "        s = greedy_replace(s, user_handle_map, pat_user_anywhere)\n",
    "        s = greedy_replace(s, experiment_map, pat_exp)\n",
    "        out[cmd_col] = s\n",
    "\n",
    "    # 3) Environment (global)\n",
    "    if env_col in row.index and pd.notna(row[env_col]):\n",
    "        s = _s(row[env_col])\n",
    "        s = greedy_replace(s, user_handle_map, pat_user_anywhere)\n",
    "        s = greedy_replace(s, experiment_map, pat_exp)\n",
    "        out[env_col] = s\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f793e63-3a14-496c-9114-6f944a98130e",
   "metadata": {},
   "source": [
    "#### Hide sensitive contents in txt output ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "999a7f19-7d0a-48cf-bef9-6d374da2fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_cmd_env_report(\n",
    "    df: pd.DataFrame,\n",
    "    out_path: str | Path,\n",
    "    *,\n",
    "    group_by: str | None = None,   \n",
    "    human_wrap: int = HUMAN_WRAP,\n",
    "    include_meta: bool = True,\n",
    "    meta_cols: tuple[str, ...] = (\"User\", \"JobsubClientIpAddress\",\n",
    "                                  \"CumulativeSlotTime\", \"DAG_NodesFailed\",\n",
    "                                  \"NumJobStarts\", \"NumJobCompletions\"),\n",
    "    cmd_col: str = \"Cmd\",\n",
    "    env_col: str = \"Environment\",\n",
    ") -> Path:\n",
    "\n",
    "    p = Path(out_path)\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Ensure required cols exist ( skip if missing)\n",
    "    cols_needed = set([cmd_col, env_col]) | (set(meta_cols) if include_meta else set())\n",
    "    missing = [c for c in cols_needed if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"[write_cmd_env_report] Warning: missing columns {missing}; proceeding with what exists.\")\n",
    "\n",
    "    def format_one(idx, row) -> str:\n",
    "        parts = []\n",
    "\n",
    "        # Header line\n",
    "        header = f\"— Job #{idx} —\"\n",
    "        parts.append(header)\n",
    "\n",
    "        # Meta block (compact)\n",
    "        if include_meta:\n",
    "            for c in meta_cols:\n",
    "                if c in row.index:\n",
    "                    val = _s(row[c])\n",
    "                    if c == cmd_col or c == env_col:\n",
    "                        # don't duplicate\n",
    "                        continue\n",
    "                    # keep meta short\n",
    "                    if len(val) > human_wrap:\n",
    "                        val = _wrap_block(val, human_wrap)\n",
    "                    parts.append(f\"{c}: {val}\")\n",
    "\n",
    "        # Cmd\n",
    "        if cmd_col in row.index:\n",
    "            parts.append(\"Cmd:\")\n",
    "            parts.append(indent(_wrap_block(row[cmd_col], human_wrap), \"  \"))\n",
    "\n",
    "        # Environment\n",
    "        if env_col in row.index:\n",
    "            parts.append(\"Environment:\")\n",
    "            env_pairs = _parse_env(row[env_col])\n",
    "            parts.append(_format_env_block(env_pairs, width=human_wrap, indent_spaces=2))\n",
    "\n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "    lines_out = []\n",
    "\n",
    "    title = \"Job Command & Environment Report\"\n",
    "    meta_summary = f\"Total rows: {len(df)}\"\n",
    "    lines_out += [title, meta_summary, \"=\" * max(28, len(title)), \"\"]\n",
    "\n",
    "    if group_by and group_by in df.columns:\n",
    "        for gval, gdf in df.groupby(group_by, dropna=False):\n",
    "            header = f\"## {group_by}: {_s(gval)}  (jobs: {len(gdf)})\"\n",
    "            lines_out += [header, \"-\" * len(header)]\n",
    "            for i, (_, row) in enumerate(gdf.iterrows(), start=1):\n",
    "                lines_out.append(format_one(i, row))\n",
    "                lines_out.append(\"\")  # blank line between jobs\n",
    "            lines_out.append(\"\")      # blank line between groups\n",
    "    else:\n",
    "        for i, (_, row) in enumerate(df.iterrows(), start=1):\n",
    "            lines_out.append(format_one(i, row))\n",
    "            lines_out.append(\"\")\n",
    "\n",
    "    txt = \"\\n\".join(lines_out).rstrip() + \"\\n\"\n",
    "    p.write_text(txt, encoding=\"utf-8\")\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d591f6c3-e44e-40ff-a5b0-f989fe79bd6e",
   "metadata": {},
   "source": [
    "### main writer ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2332e6-02aa-4bd9-9822-53486e781eab",
   "metadata": {},
   "source": [
    "## cmd and envior. ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ae540-7274-42da-84c3-5c8da268f421",
   "metadata": {},
   "source": [
    "#### Failed User json (Below) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "314574ec-e2dc-42f9-821d-7124a904edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def failed_users_payload(\n",
    "    df: pd.DataFrame,\n",
    "    user_mapper: GarbleTokenMapper,\n",
    "    user_col: str = USER_COL,\n",
    "    starts_col: str = NUM_STARTS_COL,\n",
    "    completions_col: str = NUM_COMPLETIONS_COL,\n",
    ") -> Dict[str, object]:\n",
    "\n",
    "    if not {user_col, starts_col, completions_col} <= set(df.columns):\n",
    "        return {\n",
    "            \"failed_users\": [],\n",
    "            \"meta\": {\n",
    "                \"distinct_failed_users\": 0,\n",
    "                \"total_failure_rows\": 0,\n",
    "                \"note\": \"Required columns missing; cannot compute failed users.\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    mask_fail = (df[completions_col].astype(\"int\") == 0) & (df[starts_col] > 0)\n",
    "    failed_df = df.loc[mask_fail, [user_col]]\n",
    "\n",
    "    # count failure rows per user\n",
    "    fail_counts = failed_df.groupby(user_col)[user_col].count().rename(\"failure_count\")\n",
    "\n",
    "    records = []\n",
    "    total_failure_rows = int(fail_counts.sum()) if not fail_counts.empty else 0\n",
    "\n",
    "    for orig_user, fcount in fail_counts.items():\n",
    "        token = user_mapper.add(str(orig_user), valid=is_valid_user(orig_user))\n",
    "        records.append({\n",
    "            \"token\": token,\n",
    "            \"failure_count\": int(fcount),\n",
    "            \"valid\": is_valid_user(orig_user),\n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"failed_users\": records,\n",
    "        \"meta\": {\n",
    "            \"distinct_failed_users\": int(len(records)),\n",
    "            \"total_failure_rows\": total_failure_rows,\n",
    "        },\n",
    "    }\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cbdaaad8-9a76-4dbd-b721-12e9ffb6fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _canonicalize_site(df: pd.DataFrame, site_col: str, requested: str, case_insensitive: bool):\n",
    "\n",
    "    if site_col not in df.columns:\n",
    "        return False, None, f\"Missing column: {site_col}\"\n",
    "\n",
    "    series = df[site_col].dropna().astype(str).map(lambda s: s.strip())\n",
    "    uniques = series.unique().tolist()\n",
    "    if not uniques:\n",
    "        return False, None, \"No sites found in data.\"\n",
    "\n",
    "    req = str(requested).strip()\n",
    "    if not req:\n",
    "        return False, None, \"Empty site argument.\"\n",
    "\n",
    "    # exact match first\n",
    "    if req in uniques:\n",
    "        return True, req, \"exact\"\n",
    "\n",
    "    # case-insensitive match\n",
    "    if case_insensitive:\n",
    "        # collect all ci-matches (could be more than one if data is messy)\n",
    "        matches = [u for u in uniques if u.casefold() == req.casefold()]\n",
    "        if len(matches) == 1:\n",
    "            return True, matches[0], \"case-insensitive\"\n",
    "        elif len(matches) > 1:\n",
    "            # ambiguous: pick first deterministically and note ambiguity\n",
    "            matches_sorted = sorted(matches)\n",
    "            return True, matches_sorted[0], f\"ambiguous ({len(matches)} ci-matches)\"\n",
    "    return False, None, \"not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e96fb69a-e7e6-48dd-a58b-6faf4960a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def site_jobs_payload(\n",
    "    df: pd.DataFrame,\n",
    "    site_name: str,                       \n",
    "    *,\n",
    "    site_col: str = \"MATCH_EXP_JOB_Site\",\n",
    "    case_insensitive: bool = True,\n",
    "    garble: bool = True,\n",
    "    user_col: str = USER_COL,\n",
    "    cmd_col: str = \"Cmd\",\n",
    "    env_col: str = \"Environment\",\n",
    ") -> Dict[str, object]:\n",
    "\n",
    "    # Validate & canonicalize\n",
    "    is_valid, canonical_site, match_note = _canonicalize_site(\n",
    "        df, site_col=site_col, requested=site_name, case_insensitive=case_insensitive\n",
    "    )\n",
    "\n",
    "    meta_common = {\n",
    "        \"requested_site\": str(site_name),\n",
    "        \"canonical_site\": canonical_site,\n",
    "        \"is_valid_site\": bool(is_valid),\n",
    "        \"site_column\": site_col,\n",
    "        \"match_note\": match_note,\n",
    "        \"garbled\": bool(garble),\n",
    "    }\n",
    "\n",
    "    if not is_valid:\n",
    "        return {\n",
    "            \"jobs_at_site\": [],\n",
    "            \"meta\": {\n",
    "                **meta_common,\n",
    "                \"total_jobs_at_site\": 0,\n",
    "                \"columns_included\": [],\n",
    "                \"note\": \"Requested site is not valid; returning empty result.\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    # Filter rows for the (canonical) site\n",
    "    series = df[site_col].astype(str).map(lambda s: s.strip())\n",
    "    mask = series == canonical_site\n",
    "    df_site = df.loc[mask].copy()\n",
    "\n",
    "    if not garble:\n",
    "        return {\n",
    "            \"jobs_at_site\": df_site.to_dict(orient=\"records\"),\n",
    "            \"meta\": {\n",
    "                **meta_common,\n",
    "                \"total_jobs_at_site\": int(len(df_site)),\n",
    "                \"columns_included\": list(df_site.columns),\n",
    "            },\n",
    "        }\n",
    "\n",
    "    # --- Garble using your previously defined helpers ---\n",
    "    user_mapper, exp_mapper, user_handle_map, experiment_map = build_sensitive_mappers_for_df(\n",
    "        df_site, user_col=user_col, env_col=env_col, user_prefix=\"UR_\", exp_prefix=\"EX_\"\n",
    "    )\n",
    "    pat_user_local = compile_greedy_sub_regex(list(user_handle_map.keys()))\n",
    "    pat_user_anywhere = pat_user_local\n",
    "    pat_exp = compile_greedy_sub_regex(list(experiment_map.keys()))\n",
    "\n",
    "    garbled_rows = []\n",
    "    for _, row in df_site.iterrows():\n",
    "        garbled_rows.append(\n",
    "            garble_row_fields(\n",
    "                row,\n",
    "                user_col=user_col,\n",
    "                cmd_col=cmd_col,\n",
    "                env_col=env_col,\n",
    "                user_handle_map=user_handle_map,\n",
    "                experiment_map=experiment_map,\n",
    "                pat_user=pat_user_local,\n",
    "                pat_user_anywhere=pat_user_anywhere,\n",
    "                pat_exp=pat_exp,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"jobs_at_site\": garbled_rows,\n",
    "        \"meta\": {\n",
    "            **meta_common,\n",
    "            \"total_jobs_at_site\": int(len(df_site)),\n",
    "            \"columns_included\": list(df_site.columns),\n",
    "            \"maps\": {\n",
    "                \"user_handles\": user_handle_map,   # {original_handle: \"UR_n\"}\n",
    "                \"experiments\": experiment_map,     # {original_exp: \"EX_n\"}\n",
    "            },\n",
    "            \"token_prefixes\": {\"user\": \"UR_\", \"experiment\": \"EX_\"},\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e5a48d-e8a0-4b86-8ed2-21c113e39bef",
   "metadata": {},
   "source": [
    "#### sites JSON ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4339a7a2-11fa-4f14-b42e-f7afc79e5eb3",
   "metadata": {},
   "source": [
    "#### Full output ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "89f112ac-05cd-4bf5-a6be-15df6be0b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_arg_parser() -> argparse.ArgumentParser:\n",
    "    p = argparse.ArgumentParser(add_help=False)  # avoid clashing with ipykernel flags\n",
    "    # IO + basics\n",
    "    p.add_argument(\"--data-dir\", default=DATA_DIR, help=\"Directory containing parquet files\")\n",
    "    p.add_argument(\"--output-dir\", default=OUTPUT_DIR, help=\"Directory to write outputs\")\n",
    "    p.add_argument(\"--wrap\", type=int, default=HUMAN_WRAP, help=\"Wrap width for text reports\")\n",
    "\n",
    "    # Columns\n",
    "    p.add_argument(\"--user-col\", default=USER_COL)\n",
    "    p.add_argument(\"--ip-col\", default=IP_COL)\n",
    "    p.add_argument(\"--site-col\", default=\"MATCH_EXP_JOB_Site\")\n",
    "    p.add_argument(\"--cmd-col\", default=\"Cmd\")\n",
    "    p.add_argument(\"--env-col\", default=\"Environment\")\n",
    "    p.add_argument(\"--starts-col\", default=NUM_STARTS_COL)\n",
    "    p.add_argument(\"--completions-col\", default=NUM_COMPLETIONS_COL)\n",
    "\n",
    "    # Site selection\n",
    "    p.add_argument(\"--site\", default=\"FermiGrid\", help=\"Requested site to export\")\n",
    "    p.add_argument(\"--case-insensitive\", action=\"store_true\", default=True,\n",
    "                   help=\"Case-insensitive site match (default ON)\")\n",
    "    p.add_argument(\"--case-sensitive\", dest=\"case_insensitive\", action=\"store_false\",\n",
    "                   help=\"Turn OFF case-insensitive site match\")\n",
    "    p.add_argument(\"--all-sites\", action=\"store_true\", help=\"Emit one JSON per site\")\n",
    "\n",
    "    # Garbling\n",
    "    p.add_argument(\"--garble\", action=\"store_true\", default=True, help=\"Turn ON garbling (default ON)\")\n",
    "    p.add_argument(\"--no-garble\", dest=\"garble\", action=\"store_false\", help=\"Turn OFF garbling\")\n",
    "    p.add_argument(\"--user-prefix\", default=\"UR_\", help=\"Prefix for user-handle tokens\")\n",
    "    p.add_argument(\"--exp-prefix\", default=\"EX_\", help=\"Prefix for experiment tokens\")\n",
    "    p.add_argument(\"--experiments\", default=\"uboone,icarus,pip2,nova,dune\",\n",
    "                   help=\"Comma-separated experiment keywords\")\n",
    "\n",
    "    # Report controls\n",
    "    p.add_argument(\"--report-file\", default=\"cmd_env_report.txt\", help=\"Output TXT report filename\")\n",
    "    p.add_argument(\"--report-group-by\", default=None, help=\"Optional column to group report by (e.g., User)\")\n",
    "    p.add_argument(\"--include-meta\", action=\"store_true\", default=True)\n",
    "    p.add_argument(\"--no-meta\", dest=\"include_meta\", action=\"store_false\")\n",
    "\n",
    "    # Standard help that won’t collide with ipykernel, if you want:\n",
    "    p.add_argument(\"-h\", \"--help\", action=\"help\", help=\"Show this help message and exit\")\n",
    "    return p\n",
    "\n",
    "def parse_args() -> argparse.Namespace:\n",
    "    # parse_known_args ignores unknown flags (like Jupyter's -f)\n",
    "    parser = build_arg_parser()\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def change_filename(s: str) -> str:\n",
    "    s = str(s or \"\").strip()\n",
    "    s = re.sub(r\"[^\\w\\-]+\", \"_\", s)\n",
    "    s = re.sub(r\"__+\", \"_\", s).strip(\"_\")\n",
    "    return s or \"site\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ba1b48-cc04-4928-ad22-d06ff2c0456b",
   "metadata": {},
   "source": [
    "### arguments w/ defeaults ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d28d6d55-9ca7-40e9-bdaf-3bdcd06cac53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ./Output/jobs_at_FermiGrid.json  (valid=True, request=FermiGrid, canonical=FermiGrid)\n",
      "Wrote: Output/cmd_env_report.txt\n",
      "\n",
      "=== Small samples ===\n",
      "users_dict sample: {\n",
      "  \"uboonepro@fnal.gov\": {\n",
      "    \"id\": \"UR1\",\n",
      "    \"count\": 99239,\n",
      "    \"valid\": true\n",
      "  },\n",
      "  \"icaruspro@fnal.gov\": {\n",
      "    \"id\": \"UR2\",\n",
      "    \"count\": 47080,\n",
      "    \"valid\": true\n",
      "  },\n",
      "  \"gputnam@fnal.gov\": {\n",
      "    \"id\": \"UR3\",\n",
      "    \"count\": 12693,\n",
      "    \"valid\": true\n",
      "  }\n",
      "}\n",
      "ips_dict sample  : {\n",
      "  \"131.225.240.146\": {\n",
      "    \"id\": \"IP1\",\n",
      "    \"count\": 86225,\n",
      "    \"valid\": true\n",
      "  },\n",
      "  \"131.225.240.90\": {\n",
      "    \"id\": \"IP2\",\n",
      "    \"count\": 47080,\n",
      "    \"valid\": true\n",
      "  },\n",
      "  \"131.225.240.140\": {\n",
      "    \"id\": \"IP3\",\n",
      "    \"count\": 12693,\n",
      "    \"valid\": true\n",
      "  }\n",
      "}\n",
      "\n",
      "summary.json preview:\n",
      " {\n",
      "  \"users\": [\n",
      "    [\n",
      "      \"UR1\",\n",
      "      99239,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR2\",\n",
      "      47080,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR3\",\n",
      "      12693,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR4\",\n",
      "      3652,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR5\",\n",
      "      15298,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR6\",\n",
      "      621,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR7\",\n",
      "      28340,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR8\",\n",
      "      1508,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR9\",\n",
      "      329,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR10\",\n",
      "      6867,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR11\",\n",
      "      417,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR12\",\n",
      "      105,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR13\",\n",
      "      52,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR14\",\n",
      "      42,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR15\",\n",
      "      108,\n",
      "      true\n",
      "    ],\n",
      "    [\n",
      "      \"UR16\",\n",
      "      17,\n",
      "      true\n",
      "    ],\n",
      " ...\n",
      "\n",
      "Job failure fraction %: 9.581%, job failure abs number: 24400\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "\n",
    "    # arg overrider (to rid def.)\n",
    "    DATA_DIR_RUNTIME = args.data_dir\n",
    "    OUTPUT_DIR_RUNTIME = args.output_dir\n",
    "    HUMAN_WRAP_RUNTIME = args.wrap\n",
    "\n",
    "    USER_COL_RUNTIME = args.user_col\n",
    "    IP_COL_RUNTIME = args.ip_col\n",
    "    SITE_COL_RUNTIME = args.site_col\n",
    "    CMD_COL_RUNTIME = args.cmd_col\n",
    "    ENV_COL_RUNTIME = args.env_col\n",
    "    STARTS_COL_RUNTIME = args.starts_col\n",
    "    COMPLETIONS_COL_RUNTIME = args.completions_col\n",
    "\n",
    "    # overrides global set\n",
    "    global KNOWN_EXPERIMENTS\n",
    "    KNOWN_EXPERIMENTS = set([x.strip() for x in args.experiments.split(\",\") if x.strip()])\n",
    "\n",
    "    # Ensures output dir exists\n",
    "    os.makedirs(OUTPUT_DIR_RUNTIME, exist_ok=True)\n",
    "\n",
    "    # Load data\n",
    "    df = load_dataframe(DATA_DIR_RUNTIME)\n",
    "\n",
    "    # Build obfuscations \n",
    "    users_dict, ips_dict, user_mapper, ip_mapper = build_obfuscations(\n",
    "        df, user_col=USER_COL_RUNTIME, ip_col=IP_COL_RUNTIME\n",
    "    )\n",
    "\n",
    "    # payload creators with json\n",
    "    summary_obj = make_summary_payload(df, users_dict, ips_dict,\n",
    "                                       user_col=USER_COL_RUNTIME, ip_col=IP_COL_RUNTIME)\n",
    "    summary_json = json.dumps(summary_obj, indent=2)\n",
    "\n",
    "    # jagged array json\n",
    "    users_jagged = to_jagged_array(users_dict)\n",
    "    ips_jagged   = to_jagged_array(ips_dict)\n",
    "    users_jagged_obj = {\n",
    "        \"users\": users_jagged,\n",
    "        \"meta\": {\n",
    "            \"distinct_users\": int(len(users_dict)),\n",
    "            \"total_rows\": int(len(df)),\n",
    "        },\n",
    "    }\n",
    "    ips_jagged_obj = {\n",
    "        \"ips\": ips_jagged,\n",
    "        \"meta\": {\n",
    "            \"distinct_ips\": int(len(ips_dict)),\n",
    "            \"total_rows\": int(len(df)),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Failed users payload\n",
    "    failed_obj = failed_users_payload(\n",
    "        df,\n",
    "        user_mapper=user_mapper,\n",
    "        user_col=USER_COL_RUNTIME,\n",
    "        starts_col=STARTS_COL_RUNTIME,\n",
    "        completions_col=COMPLETIONS_COL_RUNTIME,\n",
    "    )\n",
    "\n",
    "    # standard json output writer\n",
    "    dump_json(summary_obj, os.path.join(OUTPUT_DIR_RUNTIME, \"summary.json\"))\n",
    "    dump_json(users_jagged_obj, os.path.join(OUTPUT_DIR_RUNTIME, \"users_jagged.json\"))\n",
    "    dump_json(ips_jagged_obj, os.path.join(OUTPUT_DIR_RUNTIME, \"ips_jagged.json\"))\n",
    "    dump_json(failed_obj, os.path.join(OUTPUT_DIR_RUNTIME, \"failed_users.json\"))\n",
    "\n",
    "    # job - site json\n",
    "    def write_site_payload_for(site_req: str):\n",
    "        payload = site_jobs_payload(\n",
    "            df,\n",
    "            site_name=site_req,\n",
    "            site_col=SITE_COL_RUNTIME,\n",
    "            case_insensitive=args.case_insensitive,\n",
    "            garble=args.garble,\n",
    "            user_col=USER_COL_RUNTIME,\n",
    "            cmd_col=CMD_COL_RUNTIME,\n",
    "            env_col=ENV_COL_RUNTIME,\n",
    "        )\n",
    "        # Prefer canonical if valid\n",
    "        name_for_file = payload[\"meta\"][\"canonical_site\"] or site_req\n",
    "        out_fname = f\"jobs_at_{change_filename(name_for_file)}.json\"\n",
    "        out_path  = os.path.join(OUTPUT_DIR_RUNTIME, out_fname)\n",
    "        dump_json(payload, out_path)\n",
    "        print(f\"Wrote: {out_path}  (valid={payload['meta']['is_valid_site']}, \"\n",
    "              f\"request={site_req}, canonical={payload['meta']['canonical_site']})\")\n",
    "\n",
    "    if args.all_sites and SITE_COL_RUNTIME in df.columns:\n",
    "        sites = (\n",
    "            df[SITE_COL_RUNTIME]\n",
    "            .dropna()\n",
    "            .astype(str)\n",
    "            .map(str.strip)\n",
    "            .unique()\n",
    "            .tolist()\n",
    "        )\n",
    "        for s in sorted(sites, key=str.casefold):\n",
    "            write_site_payload_for(s)\n",
    "    else:\n",
    "        write_site_payload_for(args.site)\n",
    "\n",
    "    # Text output report \n",
    "    report_path = Path(OUTPUT_DIR_RUNTIME) / args.report_file\n",
    "    write_cmd_env_report(\n",
    "        df,\n",
    "        report_path,\n",
    "        group_by=args.report_group_by,\n",
    "        human_wrap=HUMAN_WRAP_RUNTIME,\n",
    "        include_meta=args.include_meta,\n",
    "        meta_cols=(\"User\", \"JobsubClientIpAddress\",\n",
    "                   \"CumulativeSlotTime\", \"DAG_NodesFailed\",\n",
    "                   \"NumJobStarts\", \"NumJobCompletions\"),\n",
    "        cmd_col=CMD_COL_RUNTIME,\n",
    "        env_col=ENV_COL_RUNTIME,\n",
    "    )\n",
    "    print(\"Wrote:\", report_path)\n",
    "\n",
    "    \n",
    "    print(\"\\n=== Small samples ===\")\n",
    "    print(\"users_dict sample:\", json.dumps(dict(list(users_dict.items())[:3]), indent=2))\n",
    "    print(\"ips_dict sample  :\", json.dumps(dict(list(ips_dict.items())[:3]), indent=2))\n",
    "    print(\"\\nsummary.json preview:\\n\", summary_json[:800], \"...\\n\")\n",
    "\n",
    "    \n",
    "    total_starts = int(df[STARTS_COL_RUNTIME].sum()) if STARTS_COL_RUNTIME in df else 0\n",
    "    total_completions = int(df[COMPLETIONS_COL_RUNTIME].astype(\"int\").sum()) if COMPLETIONS_COL_RUNTIME in df else 0\n",
    "    n_job_failures = total_starts - total_completions\n",
    "    job_failure_frac = (n_job_failures / total_starts) if total_starts else 0.0\n",
    "    print(f\"Job failure fraction %: {job_failure_frac:.3%}, job failure abs number: {n_job_failures}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d3efae-e9a1-4d22-8cb4-06375e6803bf",
   "metadata": {},
   "source": [
    "#### Main ####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
